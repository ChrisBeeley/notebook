[{"id":0,"href":"/notebook/docs/analyst-skills/resources/advancing_analytics_nhs/","title":"Advancing Analytics","section":"Resources","content":"Advancing Analytics in the NHS #  Defining strategic analytics and career pathways for healthcare analysts\nI have permission from The Strategy Unit to reproduce this document here. Please note that the work is not being reproduced with an open licence and if you wish to share or remix this document you should contact the authors yourselves to seek their permission.\nCharacteristics of a high functioning analytical team #   Is clear that their key role is to enhance the quality of the decision making process Has critical problem formulation skills- to help them engage with decision makers to ensure the right questions are being addressed Is technically proficient across the range of analytical project \u0026lsquo;types\u0026rsquo; and able to apply these to the questions at hand Is aware of its own limitations and establishes collaborative partnerships to supplement this Is able to influence the decision-making processes by providing reliable, relevant, well communicated analyses to decision makers to ensure decision quality Is dedicated to research and sharing knowledge; producing replicable work and evidencing new ways of working Is well-versed in the healthcare context in which it operates  Analytical projects typology #   mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) flowchart TB PS(Problem structuring) DM(Data management) subgraph Analysis DA(Descriptive analytics) EA(Explicative analytics) PA(Predictive analytics) PRA(Prescriptive analytics) EVA(Evaluative analytics) end PS--DM DM--Analysis Analysis--CR(Communicating results) CR--DQ(Enhancing decision quality) Example projects by type #  Descriptive #   Exploring mental health inpatient capacity Health service use in the last two years of life Population Health Management to identify and characterise ongoing health need for high-risk individuals shielded from COVID-19: a cross-sectional cohort study  Explicative #   Waiting times and attendance durations at English A\u0026amp;E Departments  Prescriptive #   Modelling the impact of COVID on waiting lists for planed care Nowcasting for improved management of COVID-19 acute bed capacity  Evaluative #   Evaluation of an Integrated Mental Health Liaison Service (Rapid Assessment Interface and Discharge Service) in Northern Ireland The Value of Triage during Periods of Intense COVID-19 Demand: Simulation Modelling Study  "},{"id":1,"href":"/notebook/docs/analyst-skills/","title":"Analyst skills","section":"Docs","content":"Analyst skills #  I\u0026rsquo;m looking at the issue of analyst skills in the Nottinghamshire ICS with some other people at the moment. The brief is:\n Consider the types of analyst teams within the ICS and their diverse functions Identify a generic set of skills for healthcare analysts in the region Identify extra skills necessary dependent on role and organisation Identify individuals within the ICS who can provide training on these skillsets Identify gaps where the system is not self sufficient for training and procure training (preferring free training) Look at the range of external training available to the system Identify areas of training need that cannot be met internally or externally  This will quite obviously require a very large amount of information about analyst skills, careers, and training, and so I will be making notes as I go along here. There may be a couple of things that come out of this that I can\u0026rsquo;t share, thinking in particular of other people\u0026rsquo;s thoughts and ideas, but I\u0026rsquo;ll share as much as I can and hopefully this will prove a useful resource for somebody else doing the same thing in another part of the country (or world, even üòâ)\nSummary of content #  Presented here will be material concerning:\n Types of analytic team Types of analyst (TODO) A summary of the skills and activities of the analysts in the region at the time of writing A generic set of skills for analysts in each type of team A review of training available to ICS staff  Some of the other material may end up being on internal documents but it wouldn\u0026rsquo;t be of interest to those outside our organisation anyway.\n"},{"id":2,"href":"/notebook/docs/analyst-skills/resources/analytics_ics/","title":"Analytics ICS","section":"Resources","content":"Recommendations for advancing the analytical capability of the NHS and its ICS partners #  This document is still under review by NHSEI but I have permission from the Strategy Unit to reproduce it here. Please note that the work is not being reproduced with an open licence and if you wish to share or remix this document you should contact the authors yourselves to seek their permission.\nThe executive summary is a pretty powerful statement and is worth reproducing in full (note again that these conclusions are currently under review at NHSEI).\n  Adopt a vision for strategic analysis that places it as the heart of strategic decision making in health and care and is based on a clear national description of what strategic analysis covers, what a high functioning strategic analytics team should look like and the skills that are associated with that. Each system should establish a strategic analytics team, separate from BI delivery - they require different skills and different working practices ‚Äì and these analytical teams, drawn from across the system, should be led by skilled analysts. These teams should be actively networked at a regional level. The network should be supported/coordinated by an expert team that can offer analytical leadership, structured education and systematic knowledge exchange focussed on advancing the capability of the ICS teams. The network members should ‚Äòown‚Äô the network and resource this development function. The network learning programme should embrace (and help analysts to navigate) the training offers already available from ‚Äòthe market‚Äô but supplement that with network delivered, context-specific learning focussed on the craft of analysis in pursuit of advancing ‚Äòdecision quality‚Äô. The regional networks should be recognised as the lead for coordinating and providing analytical development in their areas, working collaboratively as necessary. Any national resources to support analytical development should have a direct impact on ICSs with an expectation that they are deployed through the regional networks, where there is such a desire and capability exists, with governance secured through the proposed membership model. We would advocate the promulgation of national communities of practice. Existing examples that may be drawn upon include the NHS-R Community; AnalystX, and the NHS Python Community. All training offers should be accredited by a credible national body (we suggest AphA), but this needs to be done on a phased basis to avoid disruption. A national competency framework for analysts should be agreed before the end of 2021/2, one that introduces much-needed consistency in role descriptions and grading. By April 2023, all NHS recruitment and promotion to analyst roles will be dependent on applicants meeting the required standard. Protected learning time for all analysts should be built into job descriptions and a national minimum expectation should be set of at least 10 per cent of analysts‚Äô time per week protected for learning. Additionally, 1 week per annum should be identified nationally as a dedicated learning week. Regional networks should coordinate activity in this week. The principles and practical implications (e.g. for tools needed) of open-source analysis and replicability should be actively promoted and widely understood across NHS leadership. Set an expectation that ICS leaders should be analytically confident/capable and should be actively engaged in analytical development. Analytical competencies should feature in person specifications for key NHS leadership positions. Position regional networks as the prime support for this aspect of leadership development.   There are several points from the executive summary that are particularly relevant for the present task and which will be discussed in more detail following.\nAll training offers should be accredited by a credible national body #  A national competency framework for analysts should be agreed before the end of 2021/2, one that introduces much-needed consistency in role descriptions and grading #  Protected learning time for all analysts should be built into job descriptions #  \u0026hellip;a national minimum expectation should be set of at least 10 per cent of analysts‚Äô time per week protected for learning. Additionally, 1 week per annum should be identified nationally as a dedicated learning week\nFurther, the following points of note are also raised in the report\nWe must also acknowledge that a common failure of learning programmes can be the recipient returns to a context that makes no use of what they‚Äôve learned #  Recommendations for individual analysts and their teams #   To use the descriptions of high performing, strategic analytics teams, the typology of strategic analytics and career pathways to identify their learning needs and meet these needs via available learning opportunities which includes the ‚Äúart and craft‚Äù of strategic analysis Analytical teams should develop a team learning strategy in conjunction with their regional network (as proposed) which maps out a coherent plan that combines individual development with team development Connect with regional and national analysts and decision support networks   mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) flowchart LR subgraph Analytics leader AL end subgraph Data science DS1(Data Scientist- entry level)--DS2(Data Scientist- experienced)--DS3(Data Scientist- advanced) end subgraph Analyst DA1(Data Analyst- entry level)--DA2(Data Analyst- experienced)--AL(Analytics leader) end DA2--DS2 DS2--AL "},{"id":3,"href":"/notebook/docs/datascience/service-standards/","title":"Service Standards","section":"Data science in the NHS","content":"Data science and the service standards #  Although all of these standards are relevant to data science teams I will now outline those that are most relevant within data science and briefly talk about their importance\nMake the service simple to use #  // TODO\nTeam with multidisciplinary skills and perspectives #  The digital, data, and technology provides an outline of the types of roles present within an effective digital/ data/ technology team, which comprise:\n Data job family IT operations job family Product and delivery job family Quality assurance testing (QAT) job family Technical job family User-centred design job family  All of these roles are relevant to an effective data science offering in the NHS, although depending on the size of the team and the project it may be that they interact with other teams to access some of these skills (for example, in the technical job family, which includes DevOps, or in the IT job family, which includes service desk operatives).\nFollowing is an outline of the types of skills one would expect to find within a data science team. From the data job family data engineer and data scientist are the key roles. An effective data science team would ordinarily be expected to include individuals with skills in these areas. The other skillsets in this family are data analyst and performance analyst.\nData analysts and analysts are an important adjunct to an effective data science team but will not necessarily be part of the team itself.\nData analysts:\n \u0026ldquo;apply tools and techniques for data analysis and data visualisation (including the use of business information tools) identify, collect and migrate data to and from a range of systems manage, clean, abstract and aggregate data alongside a range of analytical studies on that data manipulate and link different data sets summarise and present data and conclusions in the most appropriate format for users\u0026rdquo;  Performance analysts:\n \u0026hellip;develop performance measurement frameworks - key performance indicators (KPIs), goals, user needs and benefits - and analyse the performance of a service or product against these, adapting your approach and framework appropriately and in line with any changes\n The synergy with these roles and those of an effective data science team should be clear. Data and performance analysts take metrics and methodologies that have been developed and productionised by the data science team (as well as preexisting data reports) and deliver them across the organisation\nThey make use of dashboards, regular and ad hoc reports and ensure that these measures are used and understood throughout the organisation. For the purposes of this discussion it is enough to say that the difference between the two roles is one of emphasis- with data analysts focused on bringing stakeholders new information and insight from across the organisation and performance analysts on ensuring that performance targets are met and liaising with teams and senior managers when they find that performance is not being met in a particular area. For a fuller discussion of these roles see this discussion of the data job family.\nIt should be clear, therefore, that the work of data scientists can only be brought effectively to bear on an organisation when used in conjunction with data and performance analyst skills. Effective data science teams which do not include these skills, therefore, (which some will), should have a close and mutually supportive relationship with a team which does.\nIt is worth noting that all of these skills do not have to be within one individual in the data science team, and nor does each skill have to be found in only one member of the team. Effective teams can have individuals who have a broad skillset fulfilling several of these roles and/ or \u0026ldquo;specialists\u0026rdquo; who are highly experienced within a narrow domain and fulfil this role only in the team.\nAgile #  Agile methods are widely used in data science and they are extremely helpful in making sure that the analyses and data products that are used meet user specifications as well as ensure that they are thoroughly tested in a live context before deployment. The key message from agile methods is that:\n iterative, user-centred ways of working will help you get your service in front of real users as soon as possible. Then observing and analysing data on how they use it, and iterating the service based on what you\u0026rsquo;ve learned. Because you\u0026rsquo;re not specifying everything up front before you\u0026rsquo;ve developed an understanding of what users need, you will reduce the risk of delivering the wrong thing\n Following on from the points made previously about multidisciplinary working it is worth noting that Agile Data Science states that agile data science teams favour:\n Choosing generalists over specialists Preferring small teams over large teams  There are eleven different job roles listed within Agile Data Science, and the author argues that having these roles fulfilled by individuals whose skill spans two or three of these roles can contribute to the agility of a team and reduce the overheads that large, specialised teams can suffer from, such as the need for each role to effectively communicate with other similar roles (e.g experience designer, interaction designer, and web developer).\nNHS service standard number 6 (multidisciplinary teams) explicitly states that teams must \u0026ldquo;includes people with expertise in how services are delivered across all the relevant channels, and the back end systems the service will need to integrate with\u0026rdquo;. To make such a team small and agile, it is often preferable for individuals to span several roles, some spanning front end activities (data scientists with experience in UX and web design) and some back end activities (data scientists who can do some or all of their own data engineering).\nAgile methods are covered in more detail in /TODO.\nRespect and protect users‚Äô confidentiality and privacy #  This point is clearly highly germane in healthcare, and it\u0026rsquo;s worth noting that the rapid prototyping and lack of career specialisation inherent in much of data science do increase the risks that mistakes will be made with user data. This part of the service standard lists many obligations, including:\n actively identify security and privacy threats to the service and have a robust, proportionate approach to securing information and managing fraud risks have a plan and budget that lets you manage security during the life of the service (for example, by responding to new threats, putting controls in place and applying security patches to software) work with business and information risk teams (for example, senior information risk owners and information asset owners) to make sure the service meets security requirements and regulations without putting delivery at risk carry out appropriate vulnerability and penetration testing  Early data science work (for example dashboards which do not contain identifiable data, delivered within an organisation\u0026rsquo;s firewall) may not require strict adherence to these guidelines, but the danger is that as the scope of a programme of work widens these issues become relevant and the data science team do not take the appropriate steps to ensure that they bring on board security and governance specialists who can ensure that data is processed and delivered to endusers securely.\nMake new source code open #  Although the advantages of using open source technologies and of open sourcing new technologies have been known for decades (see, e.g. The Cathedral and the Bazaar), and government policy has encouraged the production and use of open source within the public sector for ten years or more there has been surprisingly little progress in this area. The NHS is still overwhelmingly dependent on expensive, proprietary software, giving rise to vendor lock in as well as wasting public money. Anecdotal reports suggest even that many IT departments are reluctant to install the statistical programming language R, an absurd situation given its widespread adoption and use across industry and academia.\nAt the same time, NHS organisations are still very reluctant to open source any code produced by their staff or by agencies subcontracted by NHS organisations. Open sourcing new code is important enough to warrant its own chapter (see open source, for the present time it will suffice to quote from the NHS service standard:\n \u0026ldquo;Public services are built with public money. So unless there\u0026rsquo;s a good reason not to, the code they\u0026rsquo;re based should be made available for other people to reuse and build on. Open source code can save teams duplicating effort and help them build better services faster. And publishing source code under an open licence means that you\u0026rsquo;re less likely to get locked in to working with a single supplier\u0026rdquo;\n "},{"id":4,"href":"/notebook/docs/strategy/","title":"Analytics strategy","section":"Docs","content":"Introduction #  My Trust is writing some strategy at the moment and I\u0026rsquo;m writing some stuff about analytic strategy. A lot of this stuff seems to be done in bunkers without any sharing (sound familiar? üòâ) so I thought I\u0026rsquo;d publish the notes that I\u0026rsquo;m making and whatever I write.\nInformation sources #  There are two main sources of information within this document- NHSX\u0026rsquo;s excellent Data Saves Lives and East Kent and Medway\u0026rsquo;s analytics strategy, which are both shared freely online\n Data saves lives East Kent and Medway strategy  "},{"id":5,"href":"/notebook/docs/strategy/datasaveslives/","title":"Data Saves Lives","section":"Analytics strategy","content":"Data Saves Lives #   Bringing people closer to their data  the ability, if I want, to share additional data I have collected to improve my wellbeing, such as sleep, food, exercise, and genome   Giving health and care professionals the data they need to provide the best possible care  have the data to make the right decisions and recommendations about their care all relevant information about people in my care, such as data about their sleep or physical activity, so I can have information-driven conversations about their care we will introduce legislation in due course to create a statutory duty for organisations within the health and care system to share anonymous data for the benefit of the system as a whole (ongoing)   Supporting local and national decision makers with data  plan or commission services to suit local needs, including areas that need support or improvement evaluate services and care, including safety risks and good practice manage vital management functions such as workforce planning we will pilot a data and analytics accelerator (March 2022) we will begin to make all new source code that we produce or commission open and reusable and publish it under appropriate licences to encourage further innovation (such as MIT and OGLv3, alongside suitable open datasets or dummy data) (end of 2021) we will use secondary legislation in due course to enable the proportionate sharing of data including, where appropriate, personal information for the purposes of supporting the health and care system without breaching the common law duty of confidentiality   Improving data for adult social care  (nothing for me here)   Empowering researchers with the data they need to develop life-saving treatments, models of care and insights  (nothing particular here but this is a useful general point)   Helping colleagues develop the right technical infrastructure  have quick access to the information I need to plan and run my systems effectively drive interoperability across the health and care system by: having clear and open standards making it easier to share data safely and efficiently understanding the wider data architecture so I can build and buy the right systems have clear cybersecurity guidance to make sure that my systems and the data held within them is as safe as possible Data architecture principles  All data will be validated at the point of entry to improve data quality All data will be made discoverable Data will not be duplicated All clinical data stored will be made accessible using APIs published on the API gateway People will be able to self manage any data relating to their contact details and personal preferences Organisations should be able to self-manage any data relating to them, for example locations and types of services offered Data should be digitally signed to an appropriate level     Helping developers and innovators to improve health and care  clear guidance on data partnerships which maximise benefits to citizens and the system open standards, code, APIs and systems architecture so that my innovations will easily and effectively work across the system adequate documentation of the data and the APIs, and appropriate visibility on prior work that uses them clear understanding of any regulatory, data protection, data handling and cyber security obligations, so that I know how to build these in at the beginning of my project a speedy and simple approvals process for my application to interact with health and care data, so that I can get it out to users quickly clear route maps to deploy technology at scale across the system, so that my solution has the best chance to seamlessly integrate into care pathways and frontline ways of working publish a digital playbook on how to open source your code for health and care organisations with guidance on where to put the code, how to license and what licences to use, how to maintain and case studies of teams who have done this (2021) collaborate with the MRC, NIHR, and UK Research and Innovation to ensure that grants for research involving health and care data follow open and reusable code principles (ongoing) support up to 100 AI companies through the AI in Health and Care Awards to achieve market authorisation and/or the real world evidence required to support long-term NHS commissioning of their technology (March 2026) make ¬£140 million of funding available through the AI in Health and Care Award to accelerate the testing and evaluation of AI technologies (2024) helping regulators develop an approach for independently validating AI technologies for screening (June 2022)    "},{"id":6,"href":"/notebook/docs/strategy/eastkentandmedway/","title":"East Kent and Medway","section":"Analytics strategy","content":"East Kent and Medway #  We will #   Develop shared health and care analytics, which will enable us to understand the health needs of the population and to estimate how we can make the biggest improvements in improving health outcomes, patient experience, cost efficiency, workforce wellbeing and reducing health inequalities Examine the bigger picture of the drivers of good health and provide an understanding of the relationship and variation between care received throughout different points in an integrated care system Design how to move from reactive care to preventative care, through the use of prescriptive rather than descriptive analytics to provide a more holistic view of a patient‚Äôs requirements and care Develop the new and collaborative ways of working across organisational health and care boundaries needed to deliver the changes that our patients and communities need  Strategic goals #   Population Health Intelligence  We will develop intelligence to plan and commission services based on what will offer the most value for individuals, considering every aspect of their health and wellbeing, proactively preventing poor health and being ready to best manage it when it happens.  Describe the whole picture of individuals‚Äô health and wellbeing, how this is likely to change in the future, and what interventions would have the most value. Identify where we can make the most impactful improvements by addressing prevention, vulnerable groups, gaps in care, inequalities and poor outcomes. Review which interventions work well to address similar problems elsewhere, as well as where local or service specific adaptations may be needed. Assess the holistic impact of different options before implementation. Evaluate continuously which care pathways do and do not work well, for whom, and why. Considers whole health pathways from prevention to end of life, including, for example, risk factors, social determinants, mental health, quality of life and health outcomes. Allows users to extract data and to build reports themselves     Intelligence for citizens  We will enable citizens to take control of their health and wellbeing through informed decision making, optimised self-care and opportunities to influence their health and care services.   Driving innovation by working with research and industry partners  We will drive world class research and collaboration at scale that is translated to patient communities so that Kent and Medway can increase the pace of innovation in how technology is adopted.   Whole-system demand and capacity intelligence for integrated care management  We will develop a system-wide view of the flow of people and service performance, to optimise the efficiency in how our services are developed and delivered. Modelling the flow of people across the Integrated Care System, Integrated Care Partnerships and Primary Care Networks in real time, from primary care to community care. Mapping capacity in real-time across the system, and balancing this against demand. Directing people to the right part of the system, to receive the right care in the most efficient way for both the patient and the health and care system. Tracking performance targets in real-time and alerting to any issues before they happen. By monitoring the drivers of performance to understand and predict issues. By 2024 using data-driven algorithms in real-time to support a virtual command and control centre, and associated live dashboards.   Intelligent decision support for clinicians and care teams  We will enable clinicians and care teams to identify people who are at risk of poor health and wellbeing, match them to the most appropriate interventions, and view personalised information on likely risks or benefits to inform shared decision making. Developing risk stratification models with acceptable levels of sensitivity and specificity, that have been validated by local clinicians. Developing models to identify not only citizens with the highest risks, but also those who are likely to have increasing risk, and those who are likely to be most impacted by available interventions. Routinely identifying missed elements of pathways of care for individuals and ensuring that those gaps are filled. Providing clinicians and care teams with personalised intelligence for each citizen, so that they can inform them of likely impacts of different care options. By 2024:  Consistent risk and impactability algorithms for that consider the whole picture of an individual‚Äôs health and wellbeing, and can be easily applied directly at the point of care by clinicians and care teams for all individuals in the population. Decision support algorithms that alert clinicians and care teams of personalised matches to intervention options based on predicted risks and benefits. Reduced unwarranted variation by providing clinicians with tools to compare their outcomes with peers.      Supporting plans #   Intelligence to support clinicians and care teams Research Evaluation Data architecture plan Data quality plan IG plan Analytical capacity plan  What can be stopped or optimised, and how we can work better   Giving people the skills they need to ask the right questions, interpret intelligence and turn it into action  "},{"id":7,"href":"/notebook/docs/strategy/nottshealthcare/","title":"Nottinghamshire Healthcare","section":"Analytics strategy","content":"Nottinghamshire Healthcare #  Given the other published materials, what should be the priority for Nottinghamshire Healthcare? There follows the key elements of the strategy followed by more detail about each.\n Population Health Intelligence Driving innovation and quality Whole system demand and capacity intelligence Intelligent decision support  Population health intelligence #   Analytics should consider:  Mental and physical wellness Factors that contribute to wellbeing such as loneliness and employment Inequalities in health outcomes and service provision   Consider impactability:  Start with a robust evidence base Evaluate what treatments work for whom Feed evaluation information back to clinicians to guide their clinical practice Robust evaluation and high quality clinical practice should feed back into research through peer reviewed publication    Driving innovation and quality #   Produce a strategy across QI, evaluation, research, audit, CDU, and applied information and facilitate joint working between them Workforce plan across clinicians, managers, and analysts in embed QI, statistical, and data science methods in everyday practice Provide a joined up service, provoke curiosity across the workforce and triage questions across the range of data and analytic services with a single point of access Analytics should be provided at a whole system level and should be easily shared, reproducible, and self service  Data and analysis should contribute to \u0026ldquo;intelligent transparency\u0026rdquo;  Accessible Comprehensible Usable Assessable- that is, open for inspection      Whole system demand and capacity intelligence #   Contribute to building capacity in the ICS to routinely carry out demand and capacity modelling Develop a system wide approach to demand and capacity, including models of patient flow and early warning systems  Intelligent decision support #   Provide legible, real time, analytically robust PROMS, CROMS, and PREMS to all staff who need them Continue to use and build on success of data science methods such as forecasting and text mining Consider implementing CogStack to support text mining of clinical notes in the EHR  Appendix A: Related areas\n Research \u0026amp; Evaluation Data architecture  Metadata and documentation of data Data should be self service where possible Supporting analytical teams to access data from the data warehouse   Analytical capacity plan  What can be stopped or optimised, and how we can work better   Workforce plan  Data skills for the whole workforce, not just analysts Consider analytic capacity at an ICS level    "},{"id":8,"href":"/notebook/docs/datascience/open-source/","title":"Open Source","section":"Data science in the NHS","content":"Open source #  Introduction #  As discussed in an earlier chapter, section 12 of the NHS service standards compels those who follow it to \u0026ldquo;Make new source code open\u0026rdquo;. Elsewhere, the NHS is being enjoined to use (as well as produce) open source software (REF). Using free software sounds like an obvious thing for a public sector body to do, and open sourcing its own code and allowing other bodies to make use of it also sounds on the surface like a sensible approach. It\u0026rsquo;s important first to understand what the words \u0026ldquo;free\u0026rdquo; and \u0026ldquo;open\u0026rdquo; actually mean.\nOpen source software licences are not well understood within the NHS, and the important distinction between copyright and licensing even less so. This chapter will cover what free and open source software is, discuss why software licensing is so important, cover the most commonly used software licences, and explain the crucial distinction between holding the copyright for a body of code and having a licence to modify and distribute it.\n\u0026ldquo;Free\u0026rdquo; and \u0026ldquo;open\u0026rdquo; #  The words free and open both have everyday meanings in English, and the word free is perhaps doubly confusing since it can refer to something that is either provided without cost (\u0026ldquo;free as in beer\u0026rdquo;) or without restriction (\u0026ldquo;free as in speech\u0026rdquo;). Confusion often arises because much of free and open source software is both free of cost and provided without restriction. However, when organisations like the Free Software Foundation (FSF) use the word free:\n \u0026hellip;you should think of \u0026ldquo;free\u0026rdquo; as in \u0026ldquo;free speech,\u0026rdquo; not as in \u0026ldquo;free beer\u0026rdquo; (https://www.gnu.org/philosophy/free-sw.en.html)\n Indeed, being able to charge money for copies of source code is part of the meaning of \u0026ldquo;free\u0026rdquo; within the definition of free software given by the Free Software Foundation (Ibid.).\nThe word open is sometimes used to mean \u0026ldquo;visible\u0026rdquo;, and sometimes in a more restrictive meaning of \u0026ldquo;free and open source\u0026rdquo;. Merely releasing code, for example on a website, and making it visible, gives nobody else the right to use, modify or distribute it. In order to avoid this confusion, truly free and open source software is often designated as such- Free and Open Source, often abbreviated to FOSS. The remainder of this chapter will focus on FOSS. Even the meaning of FOSS can be controversial, since there is some disagreement about how \u0026ldquo;free\u0026rdquo; different software licences are. It is standard practice to take the list of FOSS licences maintained by the FSF and the Open Source Initiative (OSI) as being the canonical list of free licences.\nFree and open source software #  Before considering some of the common licences it is worth understanding the different perspectives of the FSF and the OSI and how they affect for what kinds of licence they typically advocate for. The FSF was founded by Richard Stallman (RMS) who became convinced of the need for software freedom through a number of incidents that occurred when he worked at MIT\u0026rsquo;s AI laboratory in the 70s and 80s.\nThe most famous example of these incidents relates to a modification that RMS had made to a printer in use at the lab. He had modified the printer software so that it would email everyone waiting for print jobs when it jammed, so that they could go and unjam it and prevent a backlog of printing from forming after a jam. When the printer was replaced with a newer version RMS found that the source code was not provided (as it had been with the first printer); moreover when he requested the source from someone who had worked on the printer he was told that it was not available. RMS was therefore unable to modify the new printer software to send emails after a jam as he had done previously. This and other incidents of this kind convinced RMS of the need to protect what he went on to call the \u0026ldquo;four freedoms\u0026rdquo; of software:\n The freedom to run the program as you wish, for any purpose (freedom 0). The freedom to study how the program works, and change it so it does your computing as you wish (freedom 1). Access to the source code is a precondition for this. The freedom to redistribute copies so you can help others (freedom 2). The freedom to distribute copies of your modified versions to others (freedom 3). By doing this you can give the whole community a chance to benefit from your changes. Access to the source code is a precondition for this.  Non technical readers may be confused as to the designation of the first freedom as freedom 0. This is a slightly whimsical extension of the custom in most programming languages (C, for example, or Java, but not R) to begin numbering elements of an array at \u0026ldquo;0\u0026rdquo; rather than \u0026ldquo;1\u0026rdquo;.\nAny licence that meets these conditions is considered by the FSF to be free. However, the FSF prefers licences which are \u0026ldquo;copyleft\u0026rdquo;. Copyleft, which is a distorion of the word \u0026ldquo;copyright\u0026rdquo;, is a form of licensing which allows a work to be reused, adapted, and distributed, but forces individuals who make use of that right to give the same rights to any derivative works they produce . Copyleft is therefore a form of licensing which protects the rights of all of the users who use derivative works. The most famous example of a copyleft licence is the GPL, more on which later.\nCopyleft can be contrasted with \u0026ldquo;permissive\u0026rdquo; licences, which give similar rights to reuse, modify, and distribute, but do not mandate that individuals exercising those rights give the same rights over derivative works. There is some debate as to which of these types of licences is the most \u0026ldquo;free\u0026rdquo;. The FSF argue that since permissive licences allow others to take away your rights over source code, that they are less free. Others (such as the OSI) state no preference between copyleft and permissive licences and recommend both approaches depending on the project. Still others argue that since permissive licences make fewer demands of individuals reusing and modifying code they are freer than copyleft licences. This debate echoes Erich Fromm\u0026rsquo;s classic distinction between \u0026ldquo;freedom from\u0026rdquo; (freedom from having your rights over source code taken away) and \u0026ldquo;freedom to\u0026rdquo; (freedom to reuse and modify code without sharing the derivative work).\nIt is not necessary for this argument to be settled here- for the current purpose it is enough to understand what copyleft licences (like the GPL) do and what permissive licences (such at the MIT licence) do and to make sure that data science teams in the NHS are able to choose the licence that is best for their individual project.\nLicences #  Permissive #  The most famous examples of permissive licences are the MIT and BSD licences. These licences allow reuse and modification but, unlike copyleft licences, also allow the code to be incorporated into a proprietary codebase without making that codebase subject to the terms of the original licence.\nMIT #  The MIT licence is one of the shortest and simplest licences, and reads as follows:\nMIT License\nCopyright (c) [year] [fullname]\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u0026ldquo;Software\u0026rdquo;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n THE SOFTWARE IS PROVIDED \u0026ldquo;AS IS\u0026rdquo;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n This licence shows all the typical features of an open source licence very clearly- it gives permission to others to use, copy, and modify the source, it ensures that the copyright and permission notice is displayed in any modified software, and it includes a notice indicating that the software is provided without warranty of any kind.\nApache 2.0 #  The Apache licence is quite a similar permissive licence. It is quite a lot longer than the MIT licence, in the main because it spells out more legal terms which are left implicit in the MIT licence. The main important difference is that it places more obligations on individuals distributing the code or modified versions to include materials from the original codebase. Specifically, it states:\n  You must give any other recipients of the Work or Derivative Works a copy of this License; and   You must cause any modified files to carry prominent notices stating that You changed the files; and You must retain, in the Source form of any Derivative Works that You distrib- ute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Deriva- tive Works; and If the Work includes a ‚ÄúNOTICE‚Äù text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribu- tion notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such addi- tional attribution notices cannot be construed as modifying the License.  As can be seen, this adds to the licence conditions of the MIT licence by forcing distributors of code to state changes that have been made, as well as to include the contennts of any NOTICE file from the original codebase.\nOther licences #  Although there are many other permissive licences, in the interest of brevity they will not be discussed further here. This is a very comprehensive list of licences (with some commentary from the perspecitve of the FSF).\nCopyleft #  Copyleft licences, as described previously, are quite different to permissive licences. They:\n impose substantial limitations on those who create and distribute derivative works based on works that use these licenses. The GNU General Public License (the GPL License) explicitly requires that derivative works be distributed under the terms of the GPL License and also that derivative works may only be permitted to be distributed under the terms of the license. The Mozilla License imposes different and less restrictive terms on the licensing of derivative works.\n [@understandingopensource]\nCopyleft licences therefore frequently find a use when the publishers of open source code want to make sure that the code in them is not relicensed and incorporated into a proprietary system. There is no prohibition on what the code is used for, or whether the distributors charge a fee, as long as the distributor of the code is prepared to open source all of the code in the derivative work, not just the piece originally licensed under the GPL. Quite obviously it is therefore extremely unpopular companies and individuals who are in the business of selling proprietary code. It is important to note that the GPL comes into effect whenever the code is distributed in some way- for example on a CD or as a download from the Internet. This distinction will become important later in this section.\nGPL #  The GPL (general public licence) is the cornerstone of the philosophy of the FSF, and it enshrines the four freedoms described previously. Many of the key projects championed by the FSF (such as the GNU Emacs editor and the GNU C Compiler, as well as the Linux Kernel) are licensed under the GPL. The licence begins with a nice summary of its terms:\n GNU GENERAL PUBLIC LICENSE\nVersion 3, 29 June 2007\nCopyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\nPreamble The GNU General Public License is a free, copyleft license for software and other kinds of works. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program\u0026ndash;to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others. For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it. For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions. Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users. Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\n The GPL, as can be seen, in this introductory section, lays out three important principles of the GPL- firstly, that it is designed to protect software freedom, including of derivative works. Secondly, any type of warranty is disavowed. Lastly, an explicit condition that patents arising from this code must themselves be licensed under the GPL. The FSF has strong views on software patents which are made very clear in this section- they state their view that software patents should not be able to restrict the freedom of software, but where they are, this licence prevents them from doing so.\nThe GPL goes on to give clearer guidance on the exact terms of the licence. For the sake of brevity it will not be discussed further here. Interested parties are directed to the link given above or for explanation on the terms to @understandingopensource.\nAGPL #  The AGPL (Affero General Public Licence) 1is more strongly copyleft than the GPL. Its status among the free software community is somewhat controversial, with some individuals bitterly opposed to it. The most important difference between the GPL and the AGPL is that the AGPL defines \u0026ldquo;distribution of code\u0026rdquo; (which, it should be remembered, is the point at which the provisions of the GPL come into force) as allowing a piece of software to be transmitted over a network. It is quite common in recent time for software companies to provide what is called \u0026ldquo;Software as a Service\u0026rdquo; (SaaS). An example would be Google Sheets which is a spreadsheet program delivered through a web browser. The user does not have to download any extra code to use it and therefore Google does not have to distribute the code. Software licensed under the GPL can therefore be provided as SaaS without any of the provisions of the GPL applying. The AGPL, therefore, exists to allow code to be licensed so as to ensure that even if it is provided as SaaS the entity making the provision is forced to release all of the modified code under the terms of the GPL.\nLGPL #  The LGPL (lesser general public licence) is so called because it \u0026ldquo;because it does Less to protect the user‚Äôs freedom than the ordinary General Public License. It also provides other free software developers Less of an advantage over competing non-free programs.\u0026quot;\nThe LGPL is designed for software libraries rather than full applications. A library (for the benefit of non technical readers) is a computer program that is used in the service of another computer program. For example, a piece of software that plays music might incorporate several different libraries to decode different music formats (such as MP3, FLAC, or AAC). The FSF recommend the LGPL (or rather, did recommend it) in cases where there were already proprietary libraries that can be swapped in for free ones. For example, there was a free version of the C compiler (non technical readers should just not worry what this is- suffice to say that it\u0026rsquo;s pretty important to make a Linux machine work) but licensing it under the GPL would have discouraged creators of proprietary software from using it. Their response would be simply to use a proprietary compiler, which would decrease the reach of free software and would not benefit the free software community in any way. The LGPL allows a piece of software to make use of the code in a library without making all of its source code open. It therefore preserves the free software library by making it a viable choice, even if it doesn\u0026rsquo;t make other software that uses it free (source).\nTo understand where the LGPL does and does not allow the use of LGPL code in a proprietary product the FSF distinguish \u0026ldquo;works based on the library\u0026rdquo; and \u0026ldquo;works that use the library\u0026rdquo;. Works that use the library can be incorporated within a proprietary product. What this means in practice is slightly fuzzy in practice but usually this means that a body of code will communicate with the library using some sort of standardised interface (e.g. sending some sort of stream of data in a particular way) and receive back something that itself is standardised in some way (for example, music that has been decoded). The library itself is being used for its intended purpose and is not substantially modified. Works that are based on the library will usually be works that reimplement or modify the actual library itself, changing the code and making it do something different or making it work in a different way. Under the LGPL works that do this have to release all of their code exactly as though the code was licensed by the GPL (Ibid).\nMozilla public licence #  The Mozilla public licence (MPL) is like the LGPL except more weakly copyleft. Both allow the possibility over incorporating code released under their licence conditions to be used with a proprietary codebase. As we have seen, the LGPL places fairly clear restrictions on this reuse, which effectively prevent distributors of the code from modifying the original code in any way. By contrast, the MPL allows modified code to be incorporated into a proprietary codebase as long as the modifications are themselves shared, but not the rest of the code which they are distributed with.\nNHS data science and software licences #  The best software licence for a data science project will vary case by case, but there are some broad things to consider when choosing one. The most important decision to make is between permissive and copyleft licences. Permissive licences are useful to maximise the impact of something in situations where there is no concern about what proprietary vendors might do with code. Releasing code under, for example, an MIT licence allows everybody, including individuals using proprietary code, a chance to use the code under that licence.\nUsing a copyleft licence is useful when there is concern about what proprietary vendors might do with a piece of code. For example, vendors could use some functionality from an open source project to make their own product more appealing, and then use that functionality to sell their product to more customers, and then use that market leverage to help them acquire vendor lock in. Vendor lock in is the state in which using a company\u0026rsquo;s prodcuts ensures that you find it very difficult to move to another company\u0026rsquo;s products. An example might be using a proprietary statistical software package and saving data in its proprietary format, making it difficult to transfer that data to another piece of software. If proprietary software companies can use code to make the world worse in this way, then choosing a copyleft licence is an excellent way of sharing your code without allowing anybody to incorporate it into a proprietary codebase. Proprietary software companies are free to use copyleft licensed code, but are highly unlikely to do so since it means releasing all of the code that incorporates it.\n"},{"id":9,"href":"/notebook/docs/analyst-skills/team_types/","title":"Team Types","section":"Analyst skills","content":"Team types #  Summary #  Broadly, we might think of three types of analytic team within the ICS:\n Performance Data analysis/ reporting Data science Public health/ population health  The roles of these teams is discussed in more detail following. Quoted content comes from the data job family which is licensed under the OGL3.\nPerformance #  Performance teams:\n ‚Ä¶develop performance measurement frameworks - key performance indicators (KPIs), goals, user needs and benefits - and analyse the performance of a service or product against these, adapting your approach and framework appropriately and in line with any changes\n Tools and approaches\n SPC RAG ratings Board level summaries of KPIs  Data analysis and reporting #  Data analysts:\n ‚Äúapply tools and techniques for data analysis and data visualisation (including the use of business information tools), collect and migrate data to and from a range of systems manage, clean, abstract and aggregate data alongside a range of analytical studies on that data manipulate and link different data sets summarise and present data and conclusions in the most appropriate format for users‚Äù\n Tools and approaches\n PowerBI/ Qlik/ Tableau Benchmarking SPC Statistical analysis SQL  Data science #  Data scientists\n \u0026hellip;use data to identify and solve complex business problems. They have an interdisciplinary focus, using techniques and knowledge from a range of scientific and computer science disciplines (for example, statistics, analytics, machine learning)\n Tools and approaches\n R/ Python/ Julia/ SQL Time series analysis/ forecasting Statistics (especially e.g. regression models, GAMs, GEEs, etc.) Machine learning Natural language processing Open tooling/ version control  Public health/ population health #  Population Health Management:\n \u0026hellip;improves population health by data driven planning and delivery of proactive care to achieve maximum impact. It includes segmentation, stratification and impactabilty modelling to identify local ‚Äòat risk‚Äô cohorts - and, in turn, designing and targeting interventions to prevent ill-health and to improve care and support for people with ongoing health conditions and reducing unwarranted variations in outcomes\n Tools and approaches\n Segmentation Health economics Public health and epidemiological analysis Impactability modelling Population health profiling Opportunity analysis  "},{"id":10,"href":"/notebook/docs/datascience/copyright/","title":"Copyright","section":"Data science in the NHS","content":"Copyright #  Intellectual property #  Intellectual property is an umbrella term which principally refers to copyright, trademarking, design rights, and patents. Trademarks and design rights, although they can sometimes be relevant in the world of software development (for example, in the story of the Debian linux distribution rebranding Firefox to Iceweasel) are only obliquely related to the present matters of discussion and will not be explored any further here.\nUnder the Berne convention, copyright is assigned automatically to any expression (such as a piece of music, a book, an artwork, or a set of computer code). It is not necessary to apply for copyright. Where a literary, dramatic, musical or artistic work, or a film, is made by an employee in the course of her employment (\u0026ldquo;work for hire\u0026rdquo;), her employer is the first owner of any copyright in the work (unless they agree otherwise). The expression \u0026ldquo;in the course of employment\u0026rdquo; is not well defined but the crucial difference is between \u0026ldquo;contract of service\u0026rdquo; (eg as an employee) and \u0026ldquo;contract for services\u0026rdquo; (eg as a freelancer). Individuals working under a contract of service will usually maintain copyright (unless they agree otherwise).\nPatents are a very different thing, and give intellectual property over an idea. Some argue that software patents should not exist at all and there is much debate about where the boundaries should be between ideas in software that can and cannot be patented (for example in the case of non-obviousness). The ideas and debates in this area are very important and interesting in the world of software but given the reservations many free software proponents have about software patents (and the complexity and expense of acquiring and defending software patents) they are clearly not a suitable thing for NHS data science teams to acquire and will not be discussed further here.\nCopyright, on the other hand, cannot be avoided since, as discussed above, it is automatically attributed at the moment of creation to the individual creator(s) or their employing organisation. Failing to discuss this issue at the beginning of a project could lead to confusion and disagreement once a project has begun or been completed. In a sense the holder of copyright for a piece of software licensed as FOSS (whether permissively or as copyleft) is unimportant, since the software licence ensures that anybody can use the code for any purpose (subject to the restrictions of copyleft where this is applied). However, one important power that a copyright holder has on a piece of open source software is the ability to relicense. Relicensing takes place when copyright holders wish to take existing code and place it under another licence. There is a great deal of confusion about exactly how to ensure that code can be relicensed legally, but there are examples of where this has been done successfully here, here, and here. In short, it is widely believed that in order to relicense computer code it is necessary to get the approval of all, or nearly all, of the contributors. Some projects, such as the Linux kernel, have thousands of contributors and are therefore widely seen as impossible to relicense. Where only one individual or organisation holds the copyright relicensing is simple. The FSF ensures that the copyright of all of their software projects is explicitly granted to them by all contributors so they can responsibly exercise the powers that being a copyright holder grants.\nCrown copyright #  The data scientists at the Government Digital Service have no need to concern themselves with copyright in their day to day work since\n All code produced by civil servants is automatically covered by Crown Copyright\n Source.\nThe NHS, by contrast, because is a patchwork of national and local bodies (NHS England, Nottingham City CCG, Nottinghamshire Healthcare NHS Trust\u0026hellip;), can potentially produce code that is copyright to any one of hundreds of different organisations. When these bodies cooperate they could produce code licensed to a confusing patchwork of different organisations. Individuals working for the NHS should be able to cooperate on open source projects without engaging in lengthy negotiations about which organisations will hold copyright and, furthermore, it is desirable that individuals in other NHS organisations can freely contribute without worrying about whether their employer expects to see their \u0026ldquo;share\u0026rdquo; of the intellectual property. A solution to this problem would be for there to be a presumption that NHS staff releasing open source code do so under a Crown copyright licence, unless there is a reason why an exception to this rule should be made. There is precedent for assigning copyright to the crown but assigning it routinely to open source code produced by NHS staff has not been tried previously.\n"},{"id":11,"href":"/notebook/docs/analyst-skills/skills/","title":"Skills","section":"Analyst skills","content":"Skills #  A full description of a typology of skills and an associated competency framework is out of the scope of this work, nonetheless a rough framework is presented here to give structure to the discussion about the types of training that might be useful. This can be added to in the future when more national work has been carried out with regards to competency frameworks and careers in health and care analytics. There are several dimensions of interest in this context:\n Health and care domain knowledge (MH, acute, maternity\u0026hellip;) Analytic knowledge (stats, ML, R\u0026hellip;) Analytic task (demand and capacity, population health management)  Health and care domain knowledge #  It may not be useful to list these exhaustively since there are so many possibilities and levels of hierarchy that could be used to describe this, but it might be worth describing the domains broadly.\nAnalytic knowledge #  This is the level that this work most obviously concerns and warrants a reasonably comprehensive description (although not at the level forthcoming from national work on competency frameworks)\nAnalytic task #  As well as dataset type (acute, mental healthcare, etc.) there are domains of analytic task. The SAIU has been restructured to include the following interest groups:\n Demand and Capacity Population Health Management Health Inequalities Transformation and Efficiency Place based partnerships  This is thought to provide a useful structure to the current piece of work.\n"},{"id":12,"href":"/notebook/docs/analyst-skills/training/","title":"Training","section":"Analyst skills","content":"Training #  Within this piece of work the following training needs have been identified:\n TODO  A summary of available training in these areas follows.\nArea one #   TODO  Appendix A: Training providers #   AphA Health Education England Government Statistics Service NHS R Community Midlands Decision Support Network FutureNHS Healthcare Evaluation Data Kurtosis Skills Development Network Operational Research Society QA EDX WiseOwl FutureLearn Laria Udemy Jumping Rivers Population Health Exchange NIHR CLAHRC North Thames Faculty of Public Health Public Health England East Midlands AHSN Strategy Unit Software carpentry PenARC Mango  "},{"id":13,"href":"/notebook/docs/datascience/","title":"Data science in the NHS","section":"Docs","content":"Data science in the NHS #  The GOV.UK service standard is widely regarded as representing good practice in delivering digital services in the public sector. It is mandatory for certain digital services within government but the latest guidance includes standards which can be adopted across the public sector regardless of whether they would require mandatory assessment. It includes fourteen standards.\nThis document examines these standards from the point of view of data science teams, and particularly data science teams within the NHS. The NHS service standard reproduces the 14 standards, and provides extra information to teams in the NHS in recognition of some of the ways that the NHS can differ from other areas of the public sector. The NHS service standard also includes 3 extra service standards, again recognising the unique environment of the NHS.\nExamples of the difference between working within the NHS and the rest of the public sector include:\n multi-disciplinary teams are not common in the NHS - by \u0026ldquo;multi-disciplinary\u0026rdquo; we mean teams made up of product and delivery managers, designers, developers, user researchers and content designers NHS delivery teams are less likely to be practising user-centred design and agile service delivery measuring outcomes is often more complex for health more products and services are commissioned locally, for example in hospital trusts, and they are more likely to rely on suppliers, long-term contracts and \u0026ldquo;off the shelf\u0026rdquo; solutions  These issues are clearly equally important in data science, particularly the last point, and I propose in this document that data science teams across the NHS begin to work more closely in line with the GOV.UK service standard, as expressed within the NHS digital service standard and including the extra three points on it. The fourteen GOV.UK points area listed following, followed by the three extra NHS points.\n Understand users and their needs in the context of health and care Work towards solving a whole problem for users Provide a joined up experience across all channels Make the service simple to use Make sure everyone can use the service Create a team that includes multidisciplinary skills and perspectives Use agile ways of working Iterate and improve frequently Respect and protect users' confidentiality and privacy Define what success looks like and be open about how your service is performing Choose the right tools and technology Make new source code open Use and contribute to open standards, common components and patterns Operate a reliable service Support a culture of care Make your service clinically safe Make your service interoperable  "},{"id":14,"href":"/notebook/docs/analyst-skills/resources/government_analysis_function/","title":"Government Analysis Function","section":"Resources","content":"Government Analysis Function (AF) Learning Curriculum #  Summary #  This document is designed for analysts across the civil service. It is available under the OGL3.\nIt is divided into five areas:\n Ability  Gain aptitude or potential to perform to the required standard ‚Äì ‚ÄúI need to learn how to do something now‚Äù   Technical  Demonstrate specific professional skills, knowledge or qualifications ‚Äì ‚ÄúI want to get better at doing something‚Äù   Behaviours  Actions and activities that people do which result in effective performance in a job ‚Äì ‚ÄúI want to change a behaviour‚Äù   Strengths  The things we do regularly, do well and that motivate us ‚Äì ‚ÄúI want to learn from the experiences and strengths of others‚Äù   Experience  The knowledge or mastery of an activity or subject gained through involvement in or exposure to it ‚Äì ‚ÄúI want to experience something that will help me grow‚Äù    70 : 20 : 10 learning model #  The learning opportunities on the Learning Curriculum are grouped according to the 70:20:10 Learning Model. This framework suggests that in the workplace:\n 70% of learning is experiential. It happens through daily tasks, challenges and practice 20% of learning is social. It happens with and through other people, like colleagues 10% of learning is formal. It happens through structured training courses and programs  This is a key insight in the current context and elsewhere and illustrates how important it is to:\n give analysts opportunities to practise their skills give analysts the opportunity to network and learn from each other  Formal training makes up a very small proportion of the value in learning.\nTechnical #  The technical part of the curriculum lists the following topics:\n Data analysis Data management Data science Data visualisation Economics Geography Modelling tools and techniques New systems and ways of working Operational research Quality assurance, validation and data linkage Software programming, tools and techniques Survey design  "},{"id":15,"href":"/notebook/docs/opensource/","title":"Opensource","section":"Docs","content":"Open source #  I constantly talk about open source and regurgigate the same links to policy and best practice over and over again so it has finally dawned on me that I should just write it down once and then, you guessed it\u0026hellip; open source it. So here it is.\n(I may expand this into a whole section in time but let\u0026rsquo;s start with the basics)\nPolicy #   Guidance on sharing code from GDS: https://www.gov.uk/service-manual/technology/making-source-code-open-and-reusable The NHS version of the above: https://service-manual.nhs.uk/service-standard/12-make-new-source-code-open Sharing code being talked about in NHSX‚Äôs data saves lives strategy document: https://www.gov.uk/government/publications/data-saves-lives-reshaping-health-and-social-care-with-data-draft/data-saves-lives-reshaping-health-and-social-care-with-data-draft#helping-developers-and-innovators-to-improve-health-and-care  Guidance #   https://www.gov.uk/government/publications/open-source-guidance/security-considerations-when-coding-in-the-open https://gds.blog.gov.uk/2017/09/04/the-benefits-of-coding-in-the-open/ https://medium.com/healthfdn-data-analytics/towards-open-health-analytics-our-guide-to-sharing-code-safely-on-github-5d1e018897cb  Awesome repos that show how it\u0026rsquo;s done #  (although note some of these are imperfect e.g. they lack licences)\n The superb, world leading coronavirus dashboard from PHE https://github.com/publichealthengland/coronavirus-dashboard An analysis of risk factors for COVID deaths across 17 million linked NHS records https://github.com/opensafely/risk-factors-research Open prescribing, an API/ dashboard for GP level prescription data https://github.com/ebmdatalab/openprescribing A tool to calculate growth centiles for children, developed by RCPCH https://github.com/rcpch/digital-growth-charts-server The NHSX/ NHSR funded R wrapper to our locally developed tool to classify text based patient experience data (this is from my team üòä) https://github.com/nhs-r-community/pxtextmineR An experimental bed allocation tool from NHSX https://github.com/nhsx/skunkworks-bed-allocation Open source SQL that produces the MHSDS from NHSE https://github.com/nhsengland/MHSDS  (This is really just a completely rushed random snapshot. If you know about something cool in open source health and social care, get in touch or make a PR ‚ò∫Ô∏è)\n"},{"id":16,"href":"/notebook/docs/analyst-skills/competency_frameworks/","title":"Competency Frameworks","section":"Analyst skills","content":"Competency frameworks #  Introduction #  APHA carried out a review of competency frameworks for health service analysts, published in 2021. There were 28 competency frameworks considered in the review, of which 10 were found to contain significant detail about analytical job roles and career progression. Some of the material from this review and from the competency frameworks is presented below.\nThey assessed four frameworks as being the highest ranked in the scoring that they designed:\n Government Analysis Career Framework NHS KSF AphA Professional Registration Criteria Civil Service Framework  They note that despite the high score the skills within the KSF relating to analytics are out of date and that framework will not be discussed any further here.\nOther notable frameworks they reviewed include:\n The Clinical Informatics Framework The digital, data, and technology profession capability framework EDISON Data science framework  The digital, data, and technology profession capability framework #  This framework is reviewed elsewhere in this notebook from the perspective of the synergy of the roles within it with the data science family.\n"},{"id":17,"href":"/notebook/docs/analyst-skills/data_collection/","title":"Data Collection","section":"Analyst skills","content":"Data collection #  Original PHE data collection #  Public Health England carried out a Team Level Population Health Intelligence Skills Mapping within Nottinghamshire in 2018. The tool mainly comprised data collection which described the number of full time staff within each organisation with skills in the following areas:\n Database design, development and operations Data sharing and information governance Data linkage Data sources for population health intelligence Database analysis Routine monitoring Analytics Predictive analytics Benchmarking and measuring variation Statistical analysis Research and evaluation skills Data science skills Data visualisation Communication to different audiences Consultancy skills Options appraisal Population health approaches  Skills were defined at four levels:\n 0: No experience of this skill group or do not need this skill set within their role (this can be all of the team) 1: Has an understanding of this group of skills 2: Using the skills outlined in this competency regularly as a core component to the work that they do 3: High level of expertise in this work area and are supervising or training/ teaching other staff within this area  Repeating this analysis was not felt to be productive on its own, however a more detailed set of data was collected which contained the original data spec- with the repeated part of that data collection shared back with PHE.\nNew data collection #  Domains were added to the data collection instrument based on the five areas of interest identified within the ICS:\n Demand and Capacity Population Health Management Health Inequalities Transformation and Efficiency Place based partnerships  One other domain was added to the data collection instrument based on discussions within the project group:\n Knowledge of healthcare domains, datasets, and clinical coding systems  Population health management was not added because it already existed within the original tool. Economic analysis was also identified as an area of interest but is covered in the original tool under Options appraisal. Arguably it would be better to disaggregate options appraisal to leave health economic analysis as a separate domain, but keeping the tool broadly in line with the previous PHE led exercise was felt to be valuable and so it was left in its original form.\nAdding time spent using skills #  As well as the extra other domains, an extra measure was added to each domain, which would assess the amount of time actually spent on the different domain areas. Estimating the actual time spent on 20+ domains was not felt to be a reasonable request and so four levels of use were defined:\n 0: Never 1: Once or twice a year 2: Monthly 3: Weekly  "},{"id":18,"href":"/notebook/docs/deidentification/","title":"Deidentification","section":"Docs","content":"Deidentification of data #  The team and I do a lot of stuff with patient experience data  and it can be tricky to reuse it without thoroughly redacting it because people put personal information in there- sometimes asking for a response, by email or telephone. It\u0026rsquo;s a bit of an unusual case because the data should be anonymous- there\u0026rsquo;s no need to identify patients in it at all, I just want the text of their experience, but in practice it isn\u0026rsquo;t.\nHaving some way of automatically redacting patient experience data to a high enough standard to reuse the data would be very useful, and to that end I made a pull request on the NHSX internship site to see if they could look at it with a PhD student.\nJonny Pearson very helpfully gave me some links to some resources to improve it and I need to digest the contents of a bit, may as well do it here in case it helps anyone else (or future me, for that matter üòÑ). We have:\n Introduction to anonymisation Anonymisation: managing data protection risk code of practice A Review of Anonymization for Healthcare Data  You may just want the links and ignore my witterings below of course üòâ.\nIntroduction to anonymisation #  At a simple level, organisations who are trying to anonymise data will mask direct identifiers. Some data, like diagnosis or medication is not identifiable. Some data, like name or phone number is identifiable, and these variables are known as direct identifiers. Masking the direct identifiers should in theory make it impossible to identify individuals, but Sweeney showed that combining quasi identifiers could lead individuals to be identified. Quasi identifiers are values that are not directly identifying on their own, but may be identifying in combination. Sweeney suggested that this kind of linkage attack can be prevented by generalising some of the data. For example, using year and month of birth rather than exact birth date.\nSweeney codified these ideas within a concept called k-identity. The k variable gives the minimum number of people who are indistinguishable from each other in the dataset. A k-anonymous dataset means that each row is indistinguishable from k-1 other rows. For example, when k is 5 each individual is indistinguishable from 5 other individuals.\nThe modern big data landscape #  In more recent times the data available about each individual has proliferated and additional controls are often required in order to thoroughly anonymise data. Controlling the environment can help with this, for example by having data accessed in a secure area where data access is monitored, or providing data in a secure way that doesn\u0026rsquo;t allow it to be recombined with other data, like a trusted research environment.\nEnvironmental controls do not generally affect the utility of data for analysis, but have an associated in cost, control of use, etc.\nSummary #   Deidentification comes at a cost- either in terms of the usefulness of the data or the cost of managing environmental access to data Deidentification can be very difficult when there is high dimensional or high frequency data It is not always obvious what will and what won\u0026rsquo;t lead to reidentification, and there are many high profile failure of deidentified data (e.g. Netflix)  Aggregation #  "},{"id":19,"href":"/notebook/docs/new_power/","title":"New Power","section":"Docs","content":"New Power/ Old Power #  Heimans and Timms (2014) discussed the differences between what they called \u0026ldquo;Old power\u0026rdquo; and \u0026ldquo;New power\u0026rdquo;. All illustrations are sourced from Harvard Business Review and are reproduced under fair use.\n"},{"id":20,"href":"/notebook/docs/analyst-skills/conclusion/","title":"Conclusion","section":"Analyst skills","content":"Conclusion #  Scope #  In order to carry out this work within a reasonable timeframe it was scoped to exclude any detailed work on career and competency frameworks (which are in any case forthcoming nationally).\n(Are we going to add any more scope to this- see Appendix A?)\nOutline #  The current piece of work is designed to carry out the following functions as presented in the introduction:\n Consider the types of analyst teams within the ICS and their diverse functions Identify a generic set of skills for healthcare analysts in the region Identify extra skills necessary dependent on role and organisation Identify individuals within the ICS who can provide training on these skillsets Identify gaps where the system is not self sufficient for training and procure training (preferring free training) Look at the range of external training available to the system Identify areas of training need that cannot be met internally or externally  These areas will each now be considered in turn.\nConsider the types of analyst teams within the ICS and their diverse functions #  The type and function of analytic teams across the ICS is clearly broad. They might be though of as falling roughly into one of four types:\n Performance Data analysis/ reporting Data science Public health/ population health  In turn those broad kinds of teams might work on different sorts of problems and with different approaches, the teams are therefore considerably more diverse in terms of skillset and function than might be concluded from looking at the four types. This is discussed further under skills.\nIdentify a generic set of skills for healthcare analysts in the region #  Due to the wide range of types of team and task in the region, generic skills are relatively few. It may be worthwhile, however, to define them very broadly and then to consider in more detail within particular contexts.\n Data visualisation  Whether or not the visualisations are presented to the enduser, all individuals who process data should have basic data viz skills, to allow them to rapidly check a dataset for outliers, conduct exploratory analyses, etc.   Software skills  In its broadest possible sense, data analysts need to have software skills, whether they be those relating to Excel, R, Python, PowerBI, and developing those skills can make analysts more efficient and more effective   Communication  Analysts need to communicate effectively, whether that be through the written and spoken presentations of data or in conversations with other staff relating to problem definition, hypothesis generation, resource allocation, or other matters   Hypothesis testing  In its broadest possible sense, analysts need to test hypotheses. This does not have to be done formally or even with a technique such as SPC but analysts should consider the truth values of statements and attempt to derive them from data    Identify extra skills necessary dependent on role and organisation #  The skills that depend on role and organisation are by their nature quite diverse. We can usefully think about analytic work in terms of five types:\n Demand and Capacity Population Health Management Health Inequalities Transformation and Efficiency Place based partnerships  These analytic tasks will be carried out within different domains:\n Social care (subdivisions?) Acute Mental health Community physical health care Primary care  (see also Appendix A for another analytic taxonomy that might merit inclusion)\nIdentify individuals within the ICS who can provide training on these skillsets #  TODO\nIdentify gaps where the system is not self sufficient for training and procure training (preferring free training) #  TODO\nLook at the range of external training available to the system #  TODO but see this summary of external training providers\nIdentify areas of training need that cannot be met internally or externally #  Appendix A: Decisions to be made #  Where is the focus?\n ICS Strategic Analytics team The strategic analytical needs of the ICS to be met by the above and contributions from partner organisation teams All analysts in an ICS meeting ICS and organisational needs  [I think largely 2 + 3, but for discussion]\nWhat types of analysis are we considering?\n clinical decision-making to help busy clinicians diagnose and manage disease innovation and change in the NHS, and to evaluate the success of new models of care and whether changes deliver the expected benefits effective board-level oversight of complex organisations and care systems better everyday management of the monitoring and improvement of the quality and efficiency of care senior decision-makers to respond better to national incentives and regulation the allocation of finite resources better understanding of how patients flow through the system new data and digital tools patients and the public in using information  Method of data collection for analyst skills. I would propose that we carry out some sort of review of individuals (could be completed by line managers) which would summarise:\n Skillset Activities (70% of the 70 : 20 : 10 rule) Areas where the analyst might form part of a community of practice (20% of the 70 : 20 : 10 rule) Areas where the analyst can share knowledge with the ICS (10% of the 70 : 20 : 10 rule)  "},{"id":21,"href":"/notebook/docs/analyst-skills/resources/","title":"Resources","section":"Analyst skills","content":"Resources #  Please note the resources are reproduced in the repo with permission and no permission to reproduce elsewhere is implied (although asking nicely usually works, in my experience üòâ).\nSources include:\n Advancing analytics in the NHS Recommendations for advancing the analytical capability of the NHS and its ICS partners Government analysis function learning curriculum  Appendix- other resources #  This appendix is to record other resources which were not examined in detail due to lack of time.\n Organizing for Success Part III: How to Organize and Staff Data Analytics Teams  "},{"id":22,"href":"/notebook/docs/hugo/","title":"Hugo","section":"Docs","content":"While I remember, here is a very quick list of things to do when you\u0026rsquo;re using Hugo to make websites so they work properly.\n Configure config.toml so that the baseURL points to the actual location- either online or in your file system  This doesn\u0026rsquo;t sound like much but it won\u0026rsquo;t render properly unless you do this- even if using hugo server does render nicely   Install dependencies if necessary with npm install  I\u0026rsquo;m using three different themes, I\u0026rsquo;ll list them all and talk about how to set them up another time\n Terminal Geek docs Notebook (this one)  "},{"id":23,"href":"/notebook/docs/github/","title":"GitHub","section":"Docs","content":"GitHub #  View the source and make a pull request.\n"}]