[{"id":0,"href":"/notebook/docs/analyst-skills/resources/advancing_analytics_nhs/","title":"Advancing Analytics","section":"Resources","content":"Advancing Analytics in the NHS #  Defining strategic analytics and career pathways for healthcare analysts\nI have permission from The Strategy Unit to reproduce this document here. Please note that the work is not being reproduced with an open licence and if you wish to share or remix this document you should contact the authors yourselves to seek their permission.\nCharacteristics of a high functioning analytical team #   Is clear that their key role is to enhance the quality of the decision making process Has critical problem formulation skills- to help them engage with decision makers to ensure the right questions are being addressed Is technically proficient across the range of analytical project \u0026lsquo;types\u0026rsquo; and able to apply these to the questions at hand Is aware of its own limitations and establishes collaborative partnerships to supplement this Is able to influence the decision-making processes by providing reliable, relevant, well communicated analyses to decision makers to ensure decision quality Is dedicated to research and sharing knowledge; producing replicable work and evidencing new ways of working Is well-versed in the healthcare context in which it operates  Analytical projects typology #   mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) flowchart TB PS(Problem structuring) DM(Data management) subgraph Analysis DA(Descriptive analytics) EA(Explicative analytics) PA(Predictive analytics) PRA(Prescriptive analytics) EVA(Evaluative analytics) end PS--DM DM--Analysis Analysis--CR(Communicating results) CR--DQ(Enhancing decision quality) Example projects by type #  Descriptive #   Exploring mental health inpatient capacity Health service use in the last two years of life Population Health Management to identify and characterise ongoing health need for high-risk individuals shielded from COVID-19: a cross-sectional cohort study  Explicative #   Waiting times and attendance durations at English A\u0026amp;E Departments  Prescriptive #   Modelling the impact of COVID on waiting lists for planed care Nowcasting for improved management of COVID-19 acute bed capacity  Evaluative #   Evaluation of an Integrated Mental Health Liaison Service (Rapid Assessment Interface and Discharge Service) in Northern Ireland The Value of Triage during Periods of Intense COVID-19 Demand: Simulation Modelling Study  "},{"id":1,"href":"/notebook/docs/analyst-skills/","title":"Analyst skills","section":"Docs","content":"Analyst skills #  I\u0026rsquo;m looking at the issue of analyst skills in the Nottinghamshire ICS with some other people at the moment. The brief is:\n Consider the types of analyst teams within the ICS and their diverse functions Identify a generic set of skills for healthcare analysts in the region Identify extra skills necessary dependent on role and organisation Identify individuals within the ICS who can provide training on these skillsets Identify gaps where the system is not self sufficient for training and procure training (preferring free training) Look at the range of external training available to the system Identify areas of training need that cannot be met internally or externally  This will quite obviously require a very large amount of information about analyst skills, careers, and training, and so I will be making notes as I go along here. There may be a couple of things that come out of this that I can\u0026rsquo;t share, thinking in particular of other people\u0026rsquo;s thoughts and ideas, but I\u0026rsquo;ll share as much as I can and hopefully this will prove a useful resource for somebody else doing the same thing in another part of the country (or world, even üòâ)\nSummary of content #  Presented here will be material concerning:\n Types of analytic team Types of analyst (TODO) A summary of the skills and activities of the analysts in the region at the time of writing A generic set of skills for analysts in each type of team A review of training available to ICS staff  Some of the other material may end up being on internal documents but it wouldn\u0026rsquo;t be of interest to those outside our organisation anyway.\n"},{"id":2,"href":"/notebook/docs/analyst-skills/resources/analytics_ics/","title":"Analytics ICS","section":"Resources","content":"Recommendations for advancing the analytical capability of the NHS and its ICS partners #  This document is still under review by NHSEI but I have permission from the Strategy Unit to reproduce it here. Please note that the work is not being reproduced with an open licence and if you wish to share or remix this document you should contact the authors yourselves to seek their permission.\nThe executive summary is a pretty powerful statement and is worth reproducing in full (note again that these conclusions are currently under review at NHSEI).\n  Adopt a vision for strategic analysis that places it as the heart of strategic decision making in health and care and is based on a clear national description of what strategic analysis covers, what a high functioning strategic analytics team should look like and the skills that are associated with that. Each system should establish a strategic analytics team, separate from BI delivery - they require different skills and different working practices ‚Äì and these analytical teams, drawn from across the system, should be led by skilled analysts. These teams should be actively networked at a regional level. The network should be supported/coordinated by an expert team that can offer analytical leadership, structured education and systematic knowledge exchange focussed on advancing the capability of the ICS teams. The network members should ‚Äòown‚Äô the network and resource this development function. The network learning programme should embrace (and help analysts to navigate) the training offers already available from ‚Äòthe market‚Äô but supplement that with network delivered, context-specific learning focussed on the craft of analysis in pursuit of advancing ‚Äòdecision quality‚Äô. The regional networks should be recognised as the lead for coordinating and providing analytical development in their areas, working collaboratively as necessary. Any national resources to support analytical development should have a direct impact on ICSs with an expectation that they are deployed through the regional networks, where there is such a desire and capability exists, with governance secured through the proposed membership model. We would advocate the promulgation of national communities of practice. Existing examples that may be drawn upon include the NHS-R Community; AnalystX, and the NHS Python Community. All training offers should be accredited by a credible national body (we suggest AphA), but this needs to be done on a phased basis to avoid disruption. A national competency framework for analysts should be agreed before the end of 2021/2, one that introduces much-needed consistency in role descriptions and grading. By April 2023, all NHS recruitment and promotion to analyst roles will be dependent on applicants meeting the required standard. Protected learning time for all analysts should be built into job descriptions and a national minimum expectation should be set of at least 10 per cent of analysts‚Äô time per week protected for learning. Additionally, 1 week per annum should be identified nationally as a dedicated learning week. Regional networks should coordinate activity in this week. The principles and practical implications (e.g. for tools needed) of open-source analysis and replicability should be actively promoted and widely understood across NHS leadership. Set an expectation that ICS leaders should be analytically confident/capable and should be actively engaged in analytical development. Analytical competencies should feature in person specifications for key NHS leadership positions. Position regional networks as the prime support for this aspect of leadership development.   There are several points from the executive summary that are particularly relevant for the present task and which will be discussed in more detail following.\nAll training offers should be accredited by a credible national body #  A national competency framework for analysts should be agreed before the end of 2021/2, one that introduces much-needed consistency in role descriptions and grading #  Protected learning time for all analysts should be built into job descriptions #  \u0026hellip;a national minimum expectation should be set of at least 10 per cent of analysts‚Äô time per week protected for learning. Additionally, 1 week per annum should be identified nationally as a dedicated learning week\nFurther, the following points of note are also raised in the report\nWe must also acknowledge that a common failure of learning programmes can be the recipient returns to a context that makes no use of what they‚Äôve learned #  Recommendations for individual analysts and their teams #   To use the descriptions of high performing, strategic analytics teams, the typology of strategic analytics and career pathways to identify their learning needs and meet these needs via available learning opportunities which includes the ‚Äúart and craft‚Äù of strategic analysis Analytical teams should develop a team learning strategy in conjunction with their regional network (as proposed) which maps out a coherent plan that combines individual development with team development Connect with regional and national analysts and decision support networks   mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) flowchart LR subgraph Analytics leader AL end subgraph Data science DS1(Data Scientist- entry level)--DS2(Data Scientist- experienced)--DS3(Data Scientist- advanced) end subgraph Analyst DA1(Data Analyst- entry level)--DA2(Data Analyst- experienced)--AL(Analytics leader) end DA2--DS2 DS2--AL "},{"id":3,"href":"/notebook/docs/data_saves_lives/improving_trust/","title":"Improving Trust","section":"Data Saves Lives","content":" We need to build on this to demonstrate that the health and care system is a trustworthy data custodian, and we will do this in 5 ways:\n Keep data safe and secure. Be open about how data is used. Ensure fair terms from data partnerships. Give the public a bigger say in how data is used. Improve the public‚Äôs access to their own data.   Keeping data secure and safe #  \u0026ldquo;By December 2022, we will work with expert partners and the public to implement secure data environments as a default across the NHS. We will do this by delivering:\n a clear public guide to secure data environments and our overarching policy guidelines for the use of secure data environments a robust accreditation regime to ensure our high standards for secure data environments are implemented. This will include not only accreditation requirements for secure data environments but also guidance and oversight on users of the environment, as well as the process to monitor and assess implementation a full technical specification, drawing on industry best practice, including requirements to ensure interoperability, cyber security and the use of privacy enhancing technologies a comprehensive roadmap to ensure all partners across the system know how to implement our framework, with clear indicative timescales and expectations for those at different readiness levels. This will include not only those delivering the transformation in the NHS but also software providers, academic and industry researchers, as well as funding agencies\u0026rdquo;  Being open about how data is used #  \u0026hellip;\nEnsuring fair terms for data partnerships #  \u0026ldquo;Providing researchers, industry, charities and other partners with access to data enables us to:\n improve the care we provide through clinical trials, and the development of treatments and medicines support colleagues to do their jobs by creating diagnostic or decision support tools and technologies that free up staff time and improve accuracy improve our health and care system by using insights from analytics platforms provide more care and invest in services by bringing funding into the system on fair terms, such as reduced costs for drugs or treatments, or a return for contributions to the development of intellectual property\u0026rdquo;  The 5 principles to help the NHS realise benefits for patients and the public in data partnerships #   Any use of NHS data, including operational data, not available in the public domain must have an explicit aim to improve the health, welfare or care of patients in the NHS, or the operation of the NHS. This may include the discovery of new treatments, diagnostics, and other scientific breakthroughs, as well as additional wider benefits. Where possible, the terms of any arrangements should include quantifiable and explicit benefits for patients that will be realised as part of the arrangement. NHS data is an important resource and NHS organisations entering into arrangements involving their data, individually or as a consortium, should ensure they agree fair terms for their organisation and for the NHS as a whole. In particular, the boards of NHS organisations should consider themselves ultimately responsible for ensuring that any arrangements entered into by their organisation are fair, including recognising and safeguarding the value of the data that is accessed and the resources that are generated as a result of the arrangement. Any arrangements agreed by NHS organisations should not undermine, inhibit or impact the ability of the NHS, at a national level, to maximise the value or use of NHS data. NHS organisations should not enter into exclusive arrangements for data held by the NHS, nor include conditions limiting any benefits from being applied at a national level, nor undermine the wider NHS digital architecture, including the free flow of data within health and care, open standards and interoperability. Any arrangements agreed by NHS organisations should be transparent and clearly communicated in order to support public trust and confidence in the NHS and wider government data policies. Any arrangements agreed by NHS organisations should fully adhere to all applicable national level legal, regulatory, privacy and security obligations, including in respect of the National Data Guardian‚Äôs data security standards, the UK Data Protection Act 2018 and the common law duty of confidentiality.  Giving the public a bigger say over how data is accessed and used #  \u0026hellip;\nImproving individuals‚Äô access to their own health and care information #  \u0026hellip;\n"},{"id":4,"href":"/notebook/docs/goldacre_review/modernising_service_analytics/","title":"Modernising Service Analytics","section":"Goldacre Review","content":"Modernising NHS service analytics #  Summary of recommendations #  [TBC]\nIntroduction #  \u0026ldquo;At present there is no formal professional body (but some outstanding grass roots organisations); very little formal structure around teaching and training, or ‚Äòcontinuing professional development‚Äô; and a general lack of clear technical job descriptions or qualifications specific to NHS service analytics that managers without technical skills can use to evaluate the skills mix that they have, or need, in their service. This stands in very striking contrast to arrangements around the many other technical professions that drive NHS activity including laboratory technicians, clinicians, nurses, physiotherapists, radiographers, and so on\u0026hellip;\u0026rdquo;\n\u0026ldquo;By contrast, other technical specialties in the NHS have a strong and diverse strategic infrastructure to guide their initial training, career pathways, supervision, recognition, ongoing training, and to help create a technical ‚Äòcommons of knowledge‚Äô around their work. This will include Royal Colleges or other professional membership organisations that are typically high status and well resourced; detailed job descriptions at a range of seniorities; formal arrangements nationally and locally around training both at entry level and for continuing professional development; and so on. Similarly other government analyst professions such as the Government Economic Service, the Government Statistical Service, and the Government Operational Research Service each have a head of profession, with clear career paths and progression opportunities, supported by well-curated continuing professional development, and the other features of a strong technical profession. These national organisations set out clear best practice guidance, offer analysts accreditation, and require analysts to adhere to a code of conduct. These models for technical work in both the NHS and government provide a clear template for future work around the NHS analysts service.\u0026rdquo;\nOrganisations supporting NHS analytics #   NHS-R community  GitHub   NHS python community  GitHub    \u0026ldquo;The fact that these organisations have run for so long, on modest resource, with comparatively large impact, demonstrates that there is a strong need in the NHS analytics profession for strategic structures, training, and so on. However they also show that this cannot be delivered solely through ‚Äòinspiration‚Äô to the profession on its own. The individuals involved in driving these organisations have achieved phenomenal outputs but they do not have the scale, voice, access and infrastructure of the substantive structures in other professions. They should be closely involved in all subsequent work around the NHS analyst service, the impressive power of smaller scale voluntary and charitable work has now been clearly demonstrated, but the limit of the voluntary model in this space has also been met.\u0026rdquo;\nRecommendations #  [this is mostly verbatim with a few edits for length]\nProfessional structures #  NHSA 1: create an NHS analyst service modelled on the Government Economic Service, Government Statistical Service, Government Operational Research Service and Government Social Research Service. These professions each have a head of profession, clear career paths and progression opportunities supported by continuing professional development. They hold their staff to high standards by setting out clear best practice guidance, offer accreditation, and set out a clear code of conduct. This service should be responsible for delivering most or all the following tasks.\nNHSA 2: create clear job descriptions for NHS analysts at a range of levels\nIn collaboration with NHS analysts, Association of Professional Health Analysts, NHS R Community, Royal Statistical Society and the Cabinet Office Central Digital and Data Office, the proposed NHS Analyst Service should create clear job descriptions for NHS analysts from entry level to head of profession. The job descriptions should be underpinned by a clear competency framework outlining the specific technical skills required to complete specific technical tasks.\nSuch tasks may include:\n data communication data analysis data management statistical modelling risk prediction service evaluation  NHSA 3. Revise Agenda for Change, and ensure technical staff are paid realistic salaries\nNHS analysts, software developers, engineers, and other technical staff should no longer be classified as ‚Äòadministrative / clerical‚Äô staff. Technical roles require their own category within the Agenda for Change pay scale framework, with their own job titles, capabilities, competencies, KPIs (Key Performance Indicators), and competitive remuneration packages. The NHS must stop expecting to pay highly skilled technical staff in data science and software development on salary scales devised for low and intermediate level IT technical support. Data scientists and software developers in the commercial sector routinely earn more than their manager, customer, or commissioner: this reflects market value, and is no different to employment of senior clinicians, accountants, lawyers, or other technical specialists by organisations. If barriers are hit when discussing offering higher salaries to senior developers with longstanding experience, the anchor point for negotiations should be NHS clinicians‚Äô salary.\nNHSA 4. Support an NHS analyst community\nNHSA 5. Develop an annual data conference for NHS service analysts\nNHSA 6. Find good staff, and empower them quickly with ‚Äòdata pioneer‚Äô fellowships\nThis should be an open competitive programme where applicants can seek resource to cover half of their salary for 3 to 5 years so that they can spend half of their time spreading and developing their working methods, teaching, developing teaching materials, or receiving analysts for supervision and mentorship on placements.\nNHSA 7. Identify 3 ‚Äòdata pioneer‚Äô analytics teams in ICSs and trusts\nTo demonstrate the power of modern open methods in NHS service analytics, NHSX should identify 3 Integrated Care Systems and/or hospital trusts where there are strong existing skills in analytics, informatics, and/or software engineering to act as data pioneer teams.\nFrom each group 2 to 4 individuals should be provided with advanced training in modern, open, computational and collaborative working methods, including RAP, with the rest of the team given training in the foundations so that they can learn from ‚Äòdoing‚Äô under the direction of the group leaders with advanced training. These data pioneer teams can lead by example, providing open documentation of their work for others to learn from, make the methods and code local service analytics more visible to the wider community, and feed into the wider programme of modernisation around the NHS analyst service. It may be useful to choose teams and individuals who are close to working with raw NHS records data, as they will have substantial internal knowledge around data management that will be widely applicable.\nNHSA 8. Commission intermittent code and analysis audits of organisations and analyst teams for service improvement\nIn collaboration with academics, and key organisations such as AphA and the NHS-R community, the proposed NHS Analyst Service or Analyst Head of Profession should commission regular code audits of all organisations that have received public funding for health data research or analysis\nThese audits should:\n follow a set methodology be published openly be used for the explicit purpose of improving performance, rather than penalising poor performance  Specific criteria should be developed in collaboration with the community but include:\n delivery and use of open code open methods open data where possible sharing insights support for CPD in work time whether staff meet job descriptions with training, continuing professional development or other proof ‚Äì good performance should be further incentivised by highlighting best practice examples  HSA 9. Create an Analytical Capability Index\nThis should be developed independently, and used nationally, to track whether individual organisations have room to improve and signal to leadership where gaps lie in their organisation, how they compare to peers, who they can learn from. Careful consideration should be given as to how best to present the results, and whether this should be made public or not. It is important that the results are only used to drive genuine improvements, and not used for arbitrary contextless performance management.\nTraining #  NHSA 10. Create an Open College for NHS Service Analysts\nThis brand will emphasise that the training is open to all interested parties, and that the analytic methods promoted are themselves modern, open approaches to data science. This Open College should contain the following activities, set out in NHSA 11 to NHSA 21.\nNHSA 11. Devise the content of a national training programme for NHS analysts: initial and CPD\nClear job descriptions and pathways must be tied to training and, where appropriate and non-onerous, proof of competencies. Health data is complex, as are health services: working as an analyst in this setting requires a range of specific knowledge around practical health data analytics, alongside more general technical skills in data management, analysis, and visualisation. The NHS Analyst Service should be tasked with devising a curriculum and training requirements for the key competencies associated with job roles, with clear recognition of existing experience or training in and outside of health, and so on. This should be facilitative rather than restrictive, and be focused on informing high quality training, rather than imposing onerous requirements to gather paperwork as proof of skills.\nNHSA 12. Oversee funding and delivery of training, both open online and one-to-one\nTraining should be an appropriate blend of openly accessible online training, such as MOOCs, accompanied by formal one-to-one or group work to support feedback, supervised practical work, and evaluations, in the situations and skillsets where this more expensive in-person training can be shown to deliver better outcomes than open online work alone.\nCPD courses should award completion certificates, proof of CPD, recognised or even required by managers, and these should be matched where relevant to competencies in analyst job descriptions. This should be overseen by a governing body and developed in close partnership with AphA, RSS (Royal Statistical Society, and the NHS-R Community.\nNHSA 13. Establish new core training for analysts\nReplace the Health Education England Graduate Management Training Scheme in health informatics and health analysis specialisms with a specific graduate training scheme in health data analysis, that should include: core training; rotation in different parts of the NHS (for example, in primary care vs. secondary care); the opportunity to specialise (for example, in data engineering vs. data management vs. data analysis); and specific training in the use of modern open computational methods. This will require funding and coordination from national arms‚Äô length bodies, local NHS organisations, national funders, NHS Leadership Academy, HEE, academic organisations (where they can demonstrate a specific commitment to practical NHS service analytics) and more.\nNHSA 14. Outline clear, non-onerous CPD training requirements for analysts\nNHSA 15. Embrace RAP and modern, open working methods\nExcel has its place, and training will be required at a range of levels for a range of skills. However, there is a clear need to move away from inappropriate use of inefficient and outdated ‚Äòpoint and click‚Äô methods for analysis, and towards a model based on Reproducible Analytical Pipelines with modern, open, collaborative approaches to data science.\nIntermediate, and advanced analyst training should focus on enabling the workforce to develop skills in modern, open, collaborative computational data science with an emphasis on reproducible analytic pipelines covering concepts and skills such as R, version control, GitHub, Jupyter notebooks, Pandas, and similar. This does not mean that everyone in the system must become an expert software developer: but it does require some changes in skillsets and emphasis. RAP has a proven track record in other parts of government and in Public Health Scotland, with a strong model for spreading change in organisations. These will be new skills for many and so training will entail more than links to external generic data science guides.\nNHSA 16. Ensure training focuses on RAP as much as Machine Learning\nThere is a tendency for training to be diverted into more exotic forms of data analysis such as Artificial Intelligence. These have their place, and there are many existing resources that meet these training needs very well outside of health analytics for those who have already developed outstanding skills in data science. However, the key unmet training need in the service is RAP, and the delivery of analytics using modern, open methods to achieve improvements in efficiency, sharing, quality, transparency, documentation, and reproducibility. This must be the priority for any training programme.\nNHSA 17. Create a technical team to house and develop continuing professional development resources\nTraining in technical skills needs to be delivered by those with technical skills in data science as applied to NHS data. It cannot be delivered by generalist data scientists alone. Training also needs to be kept up to date. The aim should always be to provide training in the most advanced computational data science skills; what these are will change over time.\nProviding a team of technical specialists with adequate funding to develop, deliver, share, and curate training, including the development of tools such as sandboxes where analysts in training can practice their coding and analysis against dummy data that reflects real NHS data, will be essential if training is to be high-quality and up to date.\nNHSA 18. Ensure all training is open by default\nThe traditional funding model for training from universities and many other providers is to charge per-attender. Wherever online training resources are commissioned they should be open by default, with all video lectures, training materials, written content, exercises, and code shared openly.\nIt is necessary to remove access barriers to knowledge and training, in a space that urgently requires up-skilling, and to avoid imposing a requirement on analytic staff to ask permission of generalist managers, who may lack technical skills themselves, for access to training budgets that require onerous engagement with bureaucracy.\nFully open access to all NHS analyst training resources will also create substantial network benefits. It will make these training resources accessible to NHS staff in adjacent specialties who wish to up-skill, including managers and clinicians, enabling them to drive forward better use of data in their own teams and organisations; and to outside elements from the public and private sector making it clearer to them how the NHS uses data to improve care, and how they can interact to offer help and support, or improve analytic work with better tools, algorithms, services, or insights.\nNHSA 19. Create and maintain a curated national open library of NHS analyst code\nHire a team of 10 people to create an open library of code and workbooks for key recurring tasks, examples of best practice, ‚Äòhow-to guides‚Äô, code for common analytical queries, codelists, variables, and so on. It must be unashamedly technical but meet the needs of staff with a range of abilities.\nThe library should be presented as a flexible open online website, with clear tagging, careful thought around discoverability of resources, and careful curation of individual resources into ‚Äòtraining arcs‚Äô. The library delivery team should be led by an editor experienced in producing good open online technical resources; it should include analysts but also include expertise in technical writing, knowledge management, online education, and publishing.\nAn MVP (Minimum Viable Product) should be created within 6 months by pulling together the best existing resources from national and local teams in close collaboration with all key stakeholders and teams already listed above. This library should be closely tied to (and feeding into) online teaching and CPD. CPD points should be provided for contributing to the library and there should be an obligation for any analyst developing code with public resources to contribute the outputs to the library for re-use.\nNHSA 20. Create training specifically for senior leaders to help them become better customers for data analysis\nNHSA 21. Commission a rapid review of medical school curricula and similar\nPlatforms and data access #  NHSA 22. Improve the provision of data analysis environments\nNHSA 23. Revise NHS IT policy for analysts to ensure it is fit for purpose\nAnalysts need to be able to use modern computational data science tools such as python, GitHub and docker on their NHS computers. Current IT policies often block the use of such tools. A similar challenge has been faced and recently overcome by the analytic community in government outside of health.\nThis must be addressed in national and local IT policies with clear statements on assurance and risk from the NHS Transformation Directorate to local decision makers, to make it the norm for work to be delivered using modern computational data science tools and avoid the apparently prevalent problem of analysts using these tools outside of the formal permissions and policies of their workplace.\nNHSA 24. Rationalise national audits, RightCare, GIRFT, and Model Health System\nNHSA 25. Make change practical\nThe NHS should identify 3 data pioneer ICSs that can move to a full TRE and RAP working style within 6 months; and 3 data pioneer national quality improvement audits (at least one within NHS England) that can move to full TRE and RAP working within 6 months.\nExternal collaborations #  NHSA 26. Commission and promote best practice on outsourcing analytics\nThe NHS Transformation Directorate should coordinate the development of Best Practice guidance on outsourcing to cover the range of scenarios where such external collaborations are and are not beneficial to the system, boilerplate contractual requirements, and red flags around working methods and delivery.\nNHSA 27. Require all outsourced or external work to comply with RAP and open working methods\nCurrently when analytic projects are outsourced to consultancies, academic collaborators or other agencies it is common for only the results to be reported, for example in a PDF or slide deck, without the accompanying methodology or code used to conduct the analysis. This prevents the NHS from error-checking the work, learning from it, or being able to replicate it internally, whether in the organisation that originally commissioned the work or elsewhere in the system. This creates duplication of work, and the loss of knowledge that can create efficient analyses and drive innovation.\nThis cannot solely be addressed by asking external partners for ‚Äòtraining‚Äô or more detailed narrative descriptions of the methods used. As discussed in the chapter on Open Methods, all NHS data management and analysis code should be accompanied by adequate technical documentation alongside the code, as required by the minimum standards of RAP, openly available for re-use and external scrutiny. All outsourced work should adhere to this requirement.\nNHSA 28. Support NHS/academic collaborations on RAP data science for NHS service improvement\nUKRI/NIHR should consider running an open funding call specifically for academic teams to collaborate with national or ICS NHS data analysis teams on using RAP and modern open data science techniques to improve the quality of NHS care, to deliver specific outputs, and to build mutual relationships and capacity building around applied analytics. The targeted outputs should be a range of Jupyter notebooks or similar with well-documented open code describing ‚Äì with appropriate technical documentation ‚Äì how NHS data was prepared, analysed, and used to identify or address opportunities to improve NHS clinical activity or outcomes.\n"},{"id":5,"href":"/notebook/docs/datascience/service-standards/","title":"Service Standards","section":"Data science in the NHS","content":"Data science and the service standards #  Although all of these standards are relevant to data science teams I will now outline those that are most relevant within data science and briefly talk about their importance\nMake the service simple to use #  // TODO\nTeam with multidisciplinary skills and perspectives #  The digital, data, and technology provides an outline of the types of roles present within an effective digital/ data/ technology team, which comprise:\n Data job family IT operations job family Product and delivery job family Quality assurance testing (QAT) job family Technical job family User-centred design job family  All of these roles are relevant to an effective data science offering in the NHS, although depending on the size of the team and the project it may be that they interact with other teams to access some of these skills (for example, in the technical job family, which includes DevOps, or in the IT job family, which includes service desk operatives).\nFollowing is an outline of the types of skills one would expect to find within a data science team. From the data job family data engineer and data scientist are the key roles. An effective data science team would ordinarily be expected to include individuals with skills in these areas. The other skillsets in this family are data analyst and performance analyst.\nData analysts and analysts are an important adjunct to an effective data science team but will not necessarily be part of the team itself.\nData analysts:\n \u0026ldquo;apply tools and techniques for data analysis and data visualisation (including the use of business information tools) identify, collect and migrate data to and from a range of systems manage, clean, abstract and aggregate data alongside a range of analytical studies on that data manipulate and link different data sets summarise and present data and conclusions in the most appropriate format for users\u0026rdquo;  Performance analysts:\n \u0026hellip;develop performance measurement frameworks - key performance indicators (KPIs), goals, user needs and benefits - and analyse the performance of a service or product against these, adapting your approach and framework appropriately and in line with any changes\n The synergy with these roles and those of an effective data science team should be clear. Data and performance analysts take metrics and methodologies that have been developed and productionised by the data science team (as well as preexisting data reports) and deliver them across the organisation\nThey make use of dashboards, regular and ad hoc reports and ensure that these measures are used and understood throughout the organisation. For the purposes of this discussion it is enough to say that the difference between the two roles is one of emphasis- with data analysts focused on bringing stakeholders new information and insight from across the organisation and performance analysts on ensuring that performance targets are met and liaising with teams and senior managers when they find that performance is not being met in a particular area. For a fuller discussion of these roles see this discussion of the data job family.\nIt should be clear, therefore, that the work of data scientists can only be brought effectively to bear on an organisation when used in conjunction with data and performance analyst skills. Effective data science teams which do not include these skills, therefore, (which some will), should have a close and mutually supportive relationship with a team which does.\nIt is worth noting that all of these skills do not have to be within one individual in the data science team, and nor does each skill have to be found in only one member of the team. Effective teams can have individuals who have a broad skillset fulfilling several of these roles and/ or \u0026ldquo;specialists\u0026rdquo; who are highly experienced within a narrow domain and fulfil this role only in the team.\nAgile #  Agile methods are widely used in data science and they are extremely helpful in making sure that the analyses and data products that are used meet user specifications as well as ensure that they are thoroughly tested in a live context before deployment. The key message from agile methods is that:\n iterative, user-centred ways of working will help you get your service in front of real users as soon as possible. Then observing and analysing data on how they use it, and iterating the service based on what you\u0026rsquo;ve learned. Because you\u0026rsquo;re not specifying everything up front before you\u0026rsquo;ve developed an understanding of what users need, you will reduce the risk of delivering the wrong thing\n Following on from the points made previously about multidisciplinary working it is worth noting that Agile Data Science states that agile data science teams favour:\n Choosing generalists over specialists Preferring small teams over large teams  There are eleven different job roles listed within Agile Data Science, and the author argues that having these roles fulfilled by individuals whose skill spans two or three of these roles can contribute to the agility of a team and reduce the overheads that large, specialised teams can suffer from, such as the need for each role to effectively communicate with other similar roles (e.g experience designer, interaction designer, and web developer).\nNHS service standard number 6 (multidisciplinary teams) explicitly states that teams must \u0026ldquo;includes people with expertise in how services are delivered across all the relevant channels, and the back end systems the service will need to integrate with\u0026rdquo;. To make such a team small and agile, it is often preferable for individuals to span several roles, some spanning front end activities (data scientists with experience in UX and web design) and some back end activities (data scientists who can do some or all of their own data engineering).\nAgile methods are covered in more detail in /TODO.\nRespect and protect users‚Äô confidentiality and privacy #  This point is clearly highly germane in healthcare, and it\u0026rsquo;s worth noting that the rapid prototyping and lack of career specialisation inherent in much of data science do increase the risks that mistakes will be made with user data. This part of the service standard lists many obligations, including:\n actively identify security and privacy threats to the service and have a robust, proportionate approach to securing information and managing fraud risks have a plan and budget that lets you manage security during the life of the service (for example, by responding to new threats, putting controls in place and applying security patches to software) work with business and information risk teams (for example, senior information risk owners and information asset owners) to make sure the service meets security requirements and regulations without putting delivery at risk carry out appropriate vulnerability and penetration testing  Early data science work (for example dashboards which do not contain identifiable data, delivered within an organisation\u0026rsquo;s firewall) may not require strict adherence to these guidelines, but the danger is that as the scope of a programme of work widens these issues become relevant and the data science team do not take the appropriate steps to ensure that they bring on board security and governance specialists who can ensure that data is processed and delivered to endusers securely.\nMake new source code open #  Although the advantages of using open source technologies and of open sourcing new technologies have been known for decades (see, e.g. The Cathedral and the Bazaar), and government policy has encouraged the production and use of open source within the public sector for ten years or more there has been surprisingly little progress in this area. The NHS is still overwhelmingly dependent on expensive, proprietary software, giving rise to vendor lock in as well as wasting public money. Anecdotal reports suggest even that many IT departments are reluctant to install the statistical programming language R, an absurd situation given its widespread adoption and use across industry and academia.\nAt the same time, NHS organisations are still very reluctant to open source any code produced by their staff or by agencies subcontracted by NHS organisations. Open sourcing new code is important enough to warrant its own chapter (see open source, for the present time it will suffice to quote from the NHS service standard:\n \u0026ldquo;Public services are built with public money. So unless there\u0026rsquo;s a good reason not to, the code they\u0026rsquo;re based should be made available for other people to reuse and build on. Open source code can save teams duplicating effort and help them build better services faster. And publishing source code under an open licence means that you\u0026rsquo;re less likely to get locked in to working with a single supplier\u0026rdquo;\n "},{"id":6,"href":"/notebook/docs/goldacre_review/tre/","title":"Trusted Research Environments","section":"Goldacre Review","content":"What is a \u0026ldquo;Trusted Research Environment\u0026rdquo;? #   In outline, a Trusted Research Environment (TRE) is a secure environment that researchers enter in order to work on the data remotely, rather than downloading it onto their own local machine. Users can extract and download the answers from their analyses ‚Äì such as results tables, or graphs ‚Äì but individual patients‚Äô data always stays within the secure environment\n \u0026ldquo;good TREs can also provide a more efficient and collaborative computational environment for all data users, and an opportunity to make modern open working methods the simple default. Once again: it has been estimated that 80% of the work for data science with NHS records is spent on data preparation; this is currently delivered in a diverse, duplicative and ad hoc fashion, falling short of minimal RAP guidance, with different teams and individuals all using different methods and tools for even basic data management work, in different ways, in different organisations and settings, to do the same or very similar tasks, often on the same national NHS datasets such as GP data or HES/SUS\u0026rdquo;\n\u0026ldquo;In short, TREs represent an unprecedented opportunity to modernise the data management and analysis work done across the system, allowing us to achieve the following benefits:\n replace hundreds of dispersed analytic siloes, data centres and working practices with a small number of broadly standardised environments that facilitate the use of modern, efficient approaches to data science reduce the number of data centres, and thereby also reduce the number of cost centres. reduce the number of attack surfaces for cybersecurity risk overcome local IT constraints that prevent analysts installing specific types of contemporary data science software by enabling analysts to conduct their analyses at a central online location rather than on multiple local bespoke machines create technical working environments where a smaller number of expert software developers can assist all colleagues nationally, using modern industry standard data science tools, packaging up the code for recurring tasks into adequately documented ‚Äúfunctions‚Äù and ‚Äúlibraries‚Äù for easy re-use facilitate the collaborative development of highly effective interactive data tools for less skilled users with Graphic User Interfaces for safe and effective use of Point and Click tools (rather than these being an inappropriate default), using commercial and open data visualisation tools as appropriate allow (and indeed require) all data curation code to be shared with all subsequent users for review, validation, re-use, and iterative modification make modern, open, collaborative, computational approaches to data analysis the norm, facilitating Reproducible Analytic Pathways rather than duplicative, diverse and inefficient approaches to data management\u0026rdquo;  What should TREs achieve? #   Preserve patients‚Äô privacy Support RAP and modern, efficient, high quality, reproducible data analysis Provide a secure computing environment Provide a performant computing environment Earn patient trust Be surrounded by good governance, and support this with relevant technical features  \u0026ldquo;Five safes\u0026rdquo; principles #   Safe projects: is this use of the data appropriate? Safe people: can the users be trusted to use the data in an appropriate manner? Safe settings: does the access facility limit unauthorised use? Safe data: is there a disclosure risk in the data itself? Safe outputs: are the statistical results non-disclosive?  Meeting needs for expert and non-expert users #  There are two broad types of users of TREs- \u0026ldquo;expert\u0026rdquo; and \u0026ldquo;non-expert\u0026rdquo; users. The methods they use to interact with TREs are quite different.\nNon experts #  Non expert users will tend to use remote desktops. This is the most common method at the moment, and it is an important option because it allows analysts to use familiar tools such as Stata and Excel. Remote desktops pose some challenges for privacy and governance since\n Activity logs are difficult to parse- only keypresses, mouse movements, and videos of the screen can be stored, and these require lengthy manual review The data that is shared is inherently more disclosive because it is readable by standard analytical packages   Remote desktop TREs also do not lend themselves to open working, or a network of users collaborating through shared modules of code: users are often not working in ways that meet RAP standards; and exporting code at all can be challenging, since that code has been written interactively while viewing the real data, and is therefore has a higher risk of accidentally containing some disclosive information\n Expert users will benefit from a very different system, that runs in a largely script based fashion. There are skills necessary in order to be able to work in this fashion (see the chapter on open working) but they bring substantial benefits:\n RAP working with shared code is actively facilitated, and indeed is the natural way to work It is possible to keep informative logs of all activity, and automate a range of efficient working practices around re-used code for core common activities of data curation and analysis It is also much more straightforward to keep informative logs, share activity logs, and implement a range of security provisions that protect patient privacy by preventing or detecting misuse  Where there are additional privacy provisions, a full TRE can justify deeper access to more detailed and complete data while still maintaining the trust of professionals, the public, and patients\n Giving users who wish to work in this kind of environment a remote desktop would not be ‚Äúeasier‚Äù: in fact it would substantially obstruct their work\n The practical components of a TRE #  The service wrapper #  \u0026ldquo;This is the set of rules, regulations, governance and customer service that surrounds a TRE. There will be a range of rules around who can access the data, the skills or certificates they may need; there will be a similar range of rules around permissioning for projects; there will be processes to evaluate compliance with these rules; there will be forms to collect the data, and administrative processes to manage them; and so on. There will be governance for the TRE as a project in itself, and a range of permissions, contracts, relationships and governance arrangements around the patient data that is being ingested. There will be public-facing material to be managed, describing activity in the TRE to a greater or lesser extent. There will also usually be an ‚Äúoutput checking service‚Äù: when an analysis is complete, and the analysts are ready to release their tables and graphs from the secure environment, this is a final manual check to ensure that no disclosive information is being accidentally sent out. This output checking work is done by staff with skills in data management and analysis: it is a rapidly growing field of work that is long overdue for methodological innovation embodied in re-usable code\u0026rdquo;\nGeneric compute and database #  \u0026ldquo;A TRE is a truly multidisciplinary project: it requires strong knowledge of governance, but also deep knowledge of technical infrastructure. However, much of this technical work presents broadly similar requirements to that in other non-health sectors around tasks such as provisioning large databases, ensuring that they are secure, ensuring they are performant, managing access permissions, and providing a computational environment where users through some sensible means can call up processor power, memory and disk storage to execute their code.\nThere is some overlap, inevitably, with domain knowledge: the team ingesting and transforming the data will need some knowledge of the specific qualities of the data they are working with, and the end-users‚Äô needs; the implementation of processes around users will entail some knowledge of the service wrapper processes. But overall, the key practical issue is as follows: the compute, database, and service design aspects of a TRE are largely generic tasks that can be largely delivered by staff who have strong generalist software and data science skills, and who are easily recruited from other sectors.\u0026rdquo;\nSubject-specific code #  \u0026ldquo;This is best understood by analogy with other fields. The data science team at the music-streaming service Spotify do innovative work with data that helps drive the usability and popularity of their subscription service. For example, they extract patterns in the listening behaviour across all their users, and then use this to provide individual users with tailored recommendations for other music they might enjoy. The Spotify data science team couldn‚Äôt buy, off the shelf, a data science environment specifically built to service the needs of ‚Äúa global music streaming service‚Äù. They implemented standard off-the-shelf tools for a general purpose data science environment. Then, within that raw environment, they needed to build their own tools, analytic approaches, workflows, data preparation work, and so on. A new arrival in the Spotify data science team today will find modules of code, libraries and packages ‚Äì some even with nice interactive interfaces ‚Äì to help them find interesting new patterns in Spotify user data. Many of these tools will feel like part of the furniture, but they were all built by their predecessors in the Spotify data science environment. Furthermore, many of these tools will not have been built to a pre-determined specification, by software developers hired to do that work to order; rather, they will emerge from a team. A single analyst might painstakingly implement a one-off analysis; if it looks like the approach will have broader use, then a more experienced developer might offer to help package it up into a function or library, with good documentation; if it becomes a commonly used approach, they might work with other analysts to create an interactive tool.\u0026rdquo;\n"},{"id":7,"href":"/notebook/docs/strategy/","title":"Analytics strategy","section":"Docs","content":"Introduction #  My Trust is writing some strategy at the moment and I\u0026rsquo;m writing some stuff about analytic strategy. A lot of this stuff seems to be done in bunkers without any sharing (sound familiar? üòâ) so I thought I\u0026rsquo;d publish the notes that I\u0026rsquo;m making and whatever I write.\nInformation sources #  There are two main sources of information within this document- NHSX\u0026rsquo;s excellent Data Saves Lives and East Kent and Medway\u0026rsquo;s analytics strategy, which are both shared freely online\n Data saves lives East Kent and Medway strategy  "},{"id":8,"href":"/notebook/docs/strategy/datasaveslives/","title":"Data Saves Lives","section":"Analytics strategy","content":"Data Saves Lives #   Bringing people closer to their data  the ability, if I want, to share additional data I have collected to improve my wellbeing, such as sleep, food, exercise, and genome   Giving health and care professionals the data they need to provide the best possible care  have the data to make the right decisions and recommendations about their care all relevant information about people in my care, such as data about their sleep or physical activity, so I can have information-driven conversations about their care we will introduce legislation in due course to create a statutory duty for organisations within the health and care system to share anonymous data for the benefit of the system as a whole (ongoing)   Supporting local and national decision makers with data  plan or commission services to suit local needs, including areas that need support or improvement evaluate services and care, including safety risks and good practice manage vital management functions such as workforce planning we will pilot a data and analytics accelerator (March 2022) we will begin to make all new source code that we produce or commission open and reusable and publish it under appropriate licences to encourage further innovation (such as MIT and OGLv3, alongside suitable open datasets or dummy data) (end of 2021) we will use secondary legislation in due course to enable the proportionate sharing of data including, where appropriate, personal information for the purposes of supporting the health and care system without breaching the common law duty of confidentiality   Improving data for adult social care  (nothing for me here)   Empowering researchers with the data they need to develop life-saving treatments, models of care and insights  (nothing particular here but this is a useful general point)   Helping colleagues develop the right technical infrastructure  have quick access to the information I need to plan and run my systems effectively drive interoperability across the health and care system by: having clear and open standards making it easier to share data safely and efficiently understanding the wider data architecture so I can build and buy the right systems have clear cybersecurity guidance to make sure that my systems and the data held within them is as safe as possible Data architecture principles  All data will be validated at the point of entry to improve data quality All data will be made discoverable Data will not be duplicated All clinical data stored will be made accessible using APIs published on the API gateway People will be able to self manage any data relating to their contact details and personal preferences Organisations should be able to self-manage any data relating to them, for example locations and types of services offered Data should be digitally signed to an appropriate level     Helping developers and innovators to improve health and care  clear guidance on data partnerships which maximise benefits to citizens and the system open standards, code, APIs and systems architecture so that my innovations will easily and effectively work across the system adequate documentation of the data and the APIs, and appropriate visibility on prior work that uses them clear understanding of any regulatory, data protection, data handling and cyber security obligations, so that I know how to build these in at the beginning of my project a speedy and simple approvals process for my application to interact with health and care data, so that I can get it out to users quickly clear route maps to deploy technology at scale across the system, so that my solution has the best chance to seamlessly integrate into care pathways and frontline ways of working publish a digital playbook on how to open source your code for health and care organisations with guidance on where to put the code, how to license and what licences to use, how to maintain and case studies of teams who have done this (2021) collaborate with the MRC, NIHR, and UK Research and Innovation to ensure that grants for research involving health and care data follow open and reusable code principles (ongoing) support up to 100 AI companies through the AI in Health and Care Awards to achieve market authorisation and/or the real world evidence required to support long-term NHS commissioning of their technology (March 2026) make ¬£140 million of funding available through the AI in Health and Care Award to accelerate the testing and evaluation of AI technologies (2024) helping regulators develop an approach for independently validating AI technologies for screening (June 2022)    "},{"id":9,"href":"/notebook/docs/strategy/eastkentandmedway/","title":"East Kent and Medway","section":"Analytics strategy","content":"East Kent and Medway #  We will #   Develop shared health and care analytics, which will enable us to understand the health needs of the population and to estimate how we can make the biggest improvements in improving health outcomes, patient experience, cost efficiency, workforce wellbeing and reducing health inequalities Examine the bigger picture of the drivers of good health and provide an understanding of the relationship and variation between care received throughout different points in an integrated care system Design how to move from reactive care to preventative care, through the use of prescriptive rather than descriptive analytics to provide a more holistic view of a patient‚Äôs requirements and care Develop the new and collaborative ways of working across organisational health and care boundaries needed to deliver the changes that our patients and communities need  Strategic goals #   Population Health Intelligence  We will develop intelligence to plan and commission services based on what will offer the most value for individuals, considering every aspect of their health and wellbeing, proactively preventing poor health and being ready to best manage it when it happens.  Describe the whole picture of individuals‚Äô health and wellbeing, how this is likely to change in the future, and what interventions would have the most value. Identify where we can make the most impactful improvements by addressing prevention, vulnerable groups, gaps in care, inequalities and poor outcomes. Review which interventions work well to address similar problems elsewhere, as well as where local or service specific adaptations may be needed. Assess the holistic impact of different options before implementation. Evaluate continuously which care pathways do and do not work well, for whom, and why. Considers whole health pathways from prevention to end of life, including, for example, risk factors, social determinants, mental health, quality of life and health outcomes. Allows users to extract data and to build reports themselves     Intelligence for citizens  We will enable citizens to take control of their health and wellbeing through informed decision making, optimised self-care and opportunities to influence their health and care services.   Driving innovation by working with research and industry partners  We will drive world class research and collaboration at scale that is translated to patient communities so that Kent and Medway can increase the pace of innovation in how technology is adopted.   Whole-system demand and capacity intelligence for integrated care management  We will develop a system-wide view of the flow of people and service performance, to optimise the efficiency in how our services are developed and delivered. Modelling the flow of people across the Integrated Care System, Integrated Care Partnerships and Primary Care Networks in real time, from primary care to community care. Mapping capacity in real-time across the system, and balancing this against demand. Directing people to the right part of the system, to receive the right care in the most efficient way for both the patient and the health and care system. Tracking performance targets in real-time and alerting to any issues before they happen. By monitoring the drivers of performance to understand and predict issues. By 2024 using data-driven algorithms in real-time to support a virtual command and control centre, and associated live dashboards.   Intelligent decision support for clinicians and care teams  We will enable clinicians and care teams to identify people who are at risk of poor health and wellbeing, match them to the most appropriate interventions, and view personalised information on likely risks or benefits to inform shared decision making. Developing risk stratification models with acceptable levels of sensitivity and specificity, that have been validated by local clinicians. Developing models to identify not only citizens with the highest risks, but also those who are likely to have increasing risk, and those who are likely to be most impacted by available interventions. Routinely identifying missed elements of pathways of care for individuals and ensuring that those gaps are filled. Providing clinicians and care teams with personalised intelligence for each citizen, so that they can inform them of likely impacts of different care options. By 2024:  Consistent risk and impactability algorithms for that consider the whole picture of an individual‚Äôs health and wellbeing, and can be easily applied directly at the point of care by clinicians and care teams for all individuals in the population. Decision support algorithms that alert clinicians and care teams of personalised matches to intervention options based on predicted risks and benefits. Reduced unwarranted variation by providing clinicians with tools to compare their outcomes with peers.      Supporting plans #   Intelligence to support clinicians and care teams Research Evaluation Data architecture plan Data quality plan IG plan Analytical capacity plan  What can be stopped or optimised, and how we can work better   Giving people the skills they need to ask the right questions, interpret intelligence and turn it into action  "},{"id":10,"href":"/notebook/docs/strategy/nottshealthcare/","title":"Nottinghamshire Healthcare","section":"Analytics strategy","content":"Nottinghamshire Healthcare #  Given the other published materials, what should be the priority for Nottinghamshire Healthcare? There follows the key elements of the strategy followed by more detail about each.\n Population Health Intelligence Driving innovation and quality Whole system demand and capacity intelligence Intelligent decision support  Population health intelligence #   Analytics should consider:  Mental and physical wellness Factors that contribute to wellbeing such as loneliness and employment Inequalities in health outcomes and service provision   Consider impactability:  Start with a robust evidence base Evaluate what treatments work for whom Feed evaluation information back to clinicians to guide their clinical practice Robust evaluation and high quality clinical practice should feed back into research through peer reviewed publication    Driving innovation and quality #   Produce a strategy across QI, evaluation, research, audit, CDU, and applied information and facilitate joint working between them Workforce plan across clinicians, managers, and analysts in embed QI, statistical, and data science methods in everyday practice Provide a joined up service, provoke curiosity across the workforce and triage questions across the range of data and analytic services with a single point of access Analytics should be provided at a whole system level and should be easily shared, reproducible, and self service  Data and analysis should contribute to \u0026ldquo;intelligent transparency\u0026rdquo;  Accessible Comprehensible Usable Assessable- that is, open for inspection      Whole system demand and capacity intelligence #   Contribute to building capacity in the ICS to routinely carry out demand and capacity modelling Develop a system wide approach to demand and capacity, including models of patient flow and early warning systems  Intelligent decision support #   Provide legible, real time, analytically robust PROMS, CROMS, and PREMS to all staff who need them Continue to use and build on success of data science methods such as forecasting and text mining Consider implementing CogStack to support text mining of clinical notes in the EHR  Appendix A: Related areas\n Research \u0026amp; Evaluation Data architecture  Metadata and documentation of data Data should be self service where possible Supporting analytical teams to access data from the data warehouse   Analytical capacity plan  What can be stopped or optimised, and how we can work better   Workforce plan  Data skills for the whole workforce, not just analysts Consider analytic capacity at an ICS level    "},{"id":11,"href":"/notebook/docs/datascience/open-source/","title":"Open Source","section":"Data science in the NHS","content":"Open source #  Introduction #  As discussed in an earlier chapter, section 12 of the NHS service standards compels those who follow it to \u0026ldquo;Make new source code open\u0026rdquo;. Elsewhere, the NHS is being enjoined to use (as well as produce) open source software (REF). Using free software sounds like an obvious thing for a public sector body to do, and open sourcing its own code and allowing other bodies to make use of it also sounds on the surface like a sensible approach. It\u0026rsquo;s important first to understand what the words \u0026ldquo;free\u0026rdquo; and \u0026ldquo;open\u0026rdquo; actually mean.\nOpen source software licences are not well understood within the NHS, and the important distinction between copyright and licensing even less so. This chapter will cover what free and open source software is, discuss why software licensing is so important, cover the most commonly used software licences, and explain the crucial distinction between holding the copyright for a body of code and having a licence to modify and distribute it.\n\u0026ldquo;Free\u0026rdquo; and \u0026ldquo;open\u0026rdquo; #  The words free and open both have everyday meanings in English, and the word free is perhaps doubly confusing since it can refer to something that is either provided without cost (\u0026ldquo;free as in beer\u0026rdquo;) or without restriction (\u0026ldquo;free as in speech\u0026rdquo;). Confusion often arises because much of free and open source software is both free of cost and provided without restriction. However, when organisations like the Free Software Foundation (FSF) use the word free:\n \u0026hellip;you should think of \u0026ldquo;free\u0026rdquo; as in \u0026ldquo;free speech,\u0026rdquo; not as in \u0026ldquo;free beer\u0026rdquo; (https://www.gnu.org/philosophy/free-sw.en.html)\n Indeed, being able to charge money for copies of source code is part of the meaning of \u0026ldquo;free\u0026rdquo; within the definition of free software given by the Free Software Foundation (Ibid.).\nThe word open is sometimes used to mean \u0026ldquo;visible\u0026rdquo;, and sometimes in a more restrictive meaning of \u0026ldquo;free and open source\u0026rdquo;. Merely releasing code, for example on a website, and making it visible, gives nobody else the right to use, modify or distribute it. In order to avoid this confusion, truly free and open source software is often designated as such- Free and Open Source, often abbreviated to FOSS. The remainder of this chapter will focus on FOSS. Even the meaning of FOSS can be controversial, since there is some disagreement about how \u0026ldquo;free\u0026rdquo; different software licences are. It is standard practice to take the list of FOSS licences maintained by the FSF and the Open Source Initiative (OSI) as being the canonical list of free licences.\nFree and open source software #  Before considering some of the common licences it is worth understanding the different perspectives of the FSF and the OSI and how they affect for what kinds of licence they typically advocate for. The FSF was founded by Richard Stallman (RMS) who became convinced of the need for software freedom through a number of incidents that occurred when he worked at MIT\u0026rsquo;s AI laboratory in the 70s and 80s.\nThe most famous example of these incidents relates to a modification that RMS had made to a printer in use at the lab. He had modified the printer software so that it would email everyone waiting for print jobs when it jammed, so that they could go and unjam it and prevent a backlog of printing from forming after a jam. When the printer was replaced with a newer version RMS found that the source code was not provided (as it had been with the first printer); moreover when he requested the source from someone who had worked on the printer he was told that it was not available. RMS was therefore unable to modify the new printer software to send emails after a jam as he had done previously. This and other incidents of this kind convinced RMS of the need to protect what he went on to call the \u0026ldquo;four freedoms\u0026rdquo; of software:\n The freedom to run the program as you wish, for any purpose (freedom 0). The freedom to study how the program works, and change it so it does your computing as you wish (freedom 1). Access to the source code is a precondition for this. The freedom to redistribute copies so you can help others (freedom 2). The freedom to distribute copies of your modified versions to others (freedom 3). By doing this you can give the whole community a chance to benefit from your changes. Access to the source code is a precondition for this.  Non technical readers may be confused as to the designation of the first freedom as freedom 0. This is a slightly whimsical extension of the custom in most programming languages (C, for example, or Java, but not R) to begin numbering elements of an array at \u0026ldquo;0\u0026rdquo; rather than \u0026ldquo;1\u0026rdquo;.\nAny licence that meets these conditions is considered by the FSF to be free. However, the FSF prefers licences which are \u0026ldquo;copyleft\u0026rdquo;. Copyleft, which is a distorion of the word \u0026ldquo;copyright\u0026rdquo;, is a form of licensing which allows a work to be reused, adapted, and distributed, but forces individuals who make use of that right to give the same rights to any derivative works they produce . Copyleft is therefore a form of licensing which protects the rights of all of the users who use derivative works. The most famous example of a copyleft licence is the GPL, more on which later.\nCopyleft can be contrasted with \u0026ldquo;permissive\u0026rdquo; licences, which give similar rights to reuse, modify, and distribute, but do not mandate that individuals exercising those rights give the same rights over derivative works. There is some debate as to which of these types of licences is the most \u0026ldquo;free\u0026rdquo;. The FSF argue that since permissive licences allow others to take away your rights over source code, that they are less free. Others (such as the OSI) state no preference between copyleft and permissive licences and recommend both approaches depending on the project. Still others argue that since permissive licences make fewer demands of individuals reusing and modifying code they are freer than copyleft licences. This debate echoes Erich Fromm\u0026rsquo;s classic distinction between \u0026ldquo;freedom from\u0026rdquo; (freedom from having your rights over source code taken away) and \u0026ldquo;freedom to\u0026rdquo; (freedom to reuse and modify code without sharing the derivative work).\nIt is not necessary for this argument to be settled here- for the current purpose it is enough to understand what copyleft licences (like the GPL) do and what permissive licences (such at the MIT licence) do and to make sure that data science teams in the NHS are able to choose the licence that is best for their individual project.\nLicences #  Permissive #  The most famous examples of permissive licences are the MIT and BSD licences. These licences allow reuse and modification but, unlike copyleft licences, also allow the code to be incorporated into a proprietary codebase without making that codebase subject to the terms of the original licence.\nMIT #  The MIT licence is one of the shortest and simplest licences, and reads as follows:\nMIT License\nCopyright (c) [year] [fullname]\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u0026ldquo;Software\u0026rdquo;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n THE SOFTWARE IS PROVIDED \u0026ldquo;AS IS\u0026rdquo;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n This licence shows all the typical features of an open source licence very clearly- it gives permission to others to use, copy, and modify the source, it ensures that the copyright and permission notice is displayed in any modified software, and it includes a notice indicating that the software is provided without warranty of any kind.\nApache 2.0 #  The Apache licence is quite a similar permissive licence. It is quite a lot longer than the MIT licence, in the main because it spells out more legal terms which are left implicit in the MIT licence. The main important difference is that it places more obligations on individuals distributing the code or modified versions to include materials from the original codebase. Specifically, it states:\n  You must give any other recipients of the Work or Derivative Works a copy of this License; and You must cause any modified files to carry prominent notices stating that You changed the files; and You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and If the Work includes a ‚ÄúNOTICE‚Äù text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.   As can be seen, this adds to the licence conditions of the MIT licence by forcing distributors of code to state changes that have been made, as well as to include the contents of any NOTICE file from the original codebase.\nOther licences #  Although there are many other permissive licences, in the interest of brevity they will not be discussed further here. This is a very comprehensive list of licences (with some commentary from the perspecitve of the FSF).\nCopyleft #  Copyleft licences, as described previously, are quite different to permissive licences. They:\n impose substantial limitations on those who create and distribute derivative works based on works that use these licenses. The GNU General Public License (the GPL License) explicitly requires that derivative works be distributed under the terms of the GPL License and also that derivative works may only be permitted to be distributed under the terms of the license. The Mozilla License imposes different and less restrictive terms on the licensing of derivative works.\n [@understandingopensource]\nCopyleft licences therefore frequently find a use when the publishers of open source code want to make sure that the code in them is not relicensed and incorporated into a proprietary system. There is no prohibition on what the code is used for, or whether the distributors charge a fee, as long as the distributor of the code is prepared to open source all of the code in the derivative work, not just the piece originally licensed under the GPL. Quite obviously it is therefore extremely unpopular companies and individuals who are in the business of selling proprietary code. It is important to note that the GPL comes into effect whenever the code is distributed in some way- for example on a CD or as a download from the Internet. This distinction will become important later in this section.\nGPL #  The GPL (general public licence) is the cornerstone of the philosophy of the FSF, and it enshrines the four freedoms described previously. Many of the key projects championed by the FSF (such as the GNU Emacs editor and the GNU C Compiler, as well as the Linux Kernel) are licensed under the GPL. The licence begins with a nice summary of its terms:\n GNU GENERAL PUBLIC LICENSE\nVersion 3, 29 June 2007\nCopyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\nPreamble The GNU General Public License is a free, copyleft license for software and other kinds of works. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program\u0026ndash;to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others. For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it. For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions. Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users. Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.\n The GPL, as can be seen, in this introductory section, lays out three important principles of the GPL- firstly, that it is designed to protect software freedom, including of derivative works. Secondly, any type of warranty is disavowed. Lastly, an explicit condition that patents arising from this code must themselves be licensed under the GPL. The FSF has strong views on software patents which are made very clear in this section- they state their view that software patents should not be able to restrict the freedom of software, but where they are, this licence prevents them from doing so.\nThe GPL goes on to give clearer guidance on the exact terms of the licence. For the sake of brevity it will not be discussed further here. Interested parties are directed to the link given above or for explanation on the terms to @understandingopensource.\nAGPL #  The AGPL (Affero General Public Licence) 1is more strongly copyleft than the GPL. Its status among the free software community is somewhat controversial, with some individuals bitterly opposed to it. The most important difference between the GPL and the AGPL is that the AGPL defines \u0026ldquo;distribution of code\u0026rdquo; (which, it should be remembered, is the point at which the provisions of the GPL come into force) as allowing a piece of software to be transmitted over a network. It is quite common in recent time for software companies to provide what is called \u0026ldquo;Software as a Service\u0026rdquo; (SaaS). An example would be Google Sheets which is a spreadsheet program delivered through a web browser. The user does not have to download any extra code to use it and therefore Google does not have to distribute the code. Software licensed under the GPL can therefore be provided as SaaS without any of the provisions of the GPL applying. The AGPL, therefore, exists to allow code to be licensed so as to ensure that even if it is provided as SaaS the entity making the provision is forced to release all of the modified code under the terms of the GPL.\nLGPL #  The LGPL (lesser general public licence) is so called because it \u0026ldquo;because it does Less to protect the user‚Äôs freedom than the ordinary General Public License. It also provides other free software developers Less of an advantage over competing non-free programs.\u0026quot;\nThe LGPL is designed for software libraries rather than full applications. A library (for the benefit of non technical readers) is a computer program that is used in the service of another computer program. For example, a piece of software that plays music might incorporate several different libraries to decode different music formats (such as MP3, FLAC, or AAC). The FSF recommend the LGPL (or rather, did recommend it) in cases where there were already proprietary libraries that can be swapped in for free ones. For example, there was a free version of the C compiler (non technical readers should just not worry what this is- suffice to say that it\u0026rsquo;s pretty important to make a Linux machine work) but licensing it under the GPL would have discouraged creators of proprietary software from using it. Their response would be simply to use a proprietary compiler, which would decrease the reach of free software and would not benefit the free software community in any way. The LGPL allows a piece of software to make use of the code in a library without making all of its source code open. It therefore preserves the free software library by making it a viable choice, even if it doesn\u0026rsquo;t make other software that uses it free (source).\nTo understand where the LGPL does and does not allow the use of LGPL code in a proprietary product the FSF distinguish \u0026ldquo;works based on the library\u0026rdquo; and \u0026ldquo;works that use the library\u0026rdquo;. Works that use the library can be incorporated within a proprietary product. What this means in practice is slightly fuzzy in practice but usually this means that a body of code will communicate with the library using some sort of standardised interface (e.g. sending some sort of stream of data in a particular way) and receive back something that itself is standardised in some way (for example, music that has been decoded). The library itself is being used for its intended purpose and is not substantially modified. Works that are based on the library will usually be works that reimplement or modify the actual library itself, changing the code and making it do something different or making it work in a different way. Under the LGPL works that do this have to release all of their code exactly as though the code was licensed by the GPL (Ibid).\nMozilla public licence #  The Mozilla public licence (MPL) is like the LGPL except more weakly copyleft. Both allow the possibility over incorporating code released under their licence conditions to be used with a proprietary codebase. As we have seen, the LGPL places fairly clear restrictions on this reuse, which effectively prevent distributors of the code from modifying the original code in any way. By contrast, the MPL allows modified code to be incorporated into a proprietary codebase as long as the modifications are themselves shared, but not the rest of the code which they are distributed with.\nNHS data science and software licences #  The best software licence for a data science project will vary case by case, but there are some broad things to consider when choosing one. The most important decision to make is between permissive and copyleft licences. Permissive licences are useful to maximise the impact of something in situations where there is no concern about what proprietary vendors might do with code. Releasing code under, for example, an MIT licence allows everybody, including individuals using proprietary code, a chance to use the code under that licence.\nUsing a copyleft licence is useful when there is concern about what proprietary vendors might do with a piece of code. For example, vendors could use some functionality from an open source project to make their own product more appealing, and then use that functionality to sell their product to more customers, and then use that market leverage to help them acquire vendor lock in. Vendor lock in is the state in which using a company\u0026rsquo;s prodcuts ensures that you find it very difficult to move to another company\u0026rsquo;s products. An example might be using a proprietary statistical software package and saving data in its proprietary format, making it difficult to transfer that data to another piece of software. If proprietary software companies can use code to make the world worse in this way, then choosing a copyleft licence is an excellent way of sharing your code without allowing anybody to incorporate it into a proprietary codebase. Proprietary software companies are free to use copyleft licensed code, but are highly unlikely to do so since it means releasing all of the code that incorporates it.\n"},{"id":12,"href":"/notebook/docs/goldacre_review/open-working/","title":"Open Working","section":"Goldacre Review","content":"Open working #  For more information about open source and software licensing in the NHS see elsewhere in this notebook.\nMyths about open working #  This section starts with mythbusting, which is very welcome because there is a lot of rubbish spoken about open code in the NHS. To summarise:\n Adopting open working practices does not mean other countries or industry can exploit intellectual property created with state funds: there should be a robust and thoughtful exceptions framework to impose commercial licenses or restrictions on review and (separately) re-use of publicly funded code, where this is actively helpful; but this closed approach should be used in a planned and deliberate fashion, where it meets national strategic objectives, not as the unplanned default approach\n  I\u0026rsquo;m not 100% clear on what is intended by this- but if it was I imagine (dual licensing) I\u0026rsquo;m not sure about the practicalities of this one   open working is fully compatible with use of commercial products: it requires only that new code and methods created for and funded by the state should be shared as default, for interoperability, quality, and efficiency. Similarly, open working does not mean that nobody is paid: simply that new code and methods are contracted from the outset as a buy-out\n  An important point. I\u0026rsquo;ve often been told verbally that the code and working out of a contractor will be shared with me- however this rarely happens in practice. This isn\u0026rsquo;t nice to have, it\u0026rsquo;s essential, and a proper open source licence should be specified in the contract. NHS-R only funds open source work, and the licence is named on the application for funding   open working does not mean that the results of every analysis must be shared openly, or in real time. The results of an analysis are separate to the code and methods used to create them. It may often be reasonable for NHS analysts to run data analyses to monitor and optimise the delivery of care, for example, without disclosing the results of all such analyses publicly in real time\n  This red herring is trotted out at every opportunity. I only ever want the code, and never the data, but \u0026ldquo;IG\u0026rdquo; is used as an excuse to keep everything hidden   Lastly, open sharing for code is not a philosophical, political, or ideological stance, but rather a practical one. Data curation and analysis is complex technical work across multiple teams, and it can only be done well where technical material (such as code, methods and documentation) is shared between those teams\u0026hellip; the people working on NHS data stretch across hundreds of diverse public and private sector organisations. Creating a closed permissions-based system to carefully police limited sharing among a huge array of individuals across all these organisations would be a vast technical and bureaucratic project, of inconceivable complexity and expense. Most importantly, this expensive approach to balancing closed working and accessibility of information would bring no clear benefit, as there is no clearly articulated need for code and methods to be withheld from wider access\n  Personally, I think code sharing is both philosophical/ political and practical, but the point is well taken  Reproducible analytical pipelines (RAP) #   data scientists or analysts who had approached health data from other sectors\u0026hellip; They repeatedly expressed how surprised they were to find that approaches regarded as standard in other parts of industry or academia ‚Äì such as sharing code, or the everyday working practices of collaborative software development ‚Äì were not yet the norm in teams working with health data\n Reproducible analytical pipelines:\n minimise manual steps, for example copy-paste, point-click or drag-drop operations. Where it is absolutely necessary to include a manual step in the process this must be documented be built using open source software for data management, analysis and visualisation which is available to anyone, preferably R or python be open to anyone for review and re-use, with all code shared openly through open standard file and code sharing platforms such as GitHub (sharing data itself is a separate issue from sharing code, as discussed below, and should be handled very differently) guarantee an audit trail using version control software (such as Git, or in services such as GitHub) which systematically track exactly who has made which changes or contributions to the code, which characters and lines were modified, when, and ‚Äì as appropriate ‚Äì why follow existing good practice for quality assurance deepen technical and quality assurance processes with code review by peers contain well-commented code and have documentation embedded and version controlled within the work, rather than saved elsewhere  The key change to RAP:\n a recognition of the need for those working with data to embrace the many norms and behaviours of the software development community when writing code\n \u0026ldquo;There is undoubtedly a place for traditional and less technical approaches to collaboration and information management such as emails, meetings, phone calls, or written manuals. But these are ill-suited to managing long complex interdependent technical work with code and data\u0026rdquo;\nThese practices include (see the report for a description of them):\n Version control and GitHub Code review Functions Unit tests Libraries Documentation Managing the environment  Who needs these skills #  Not everyone needs these skills, there are important other roles\u0026hellip;\n\u0026ldquo;individuals with good skills around interpreting data, and communicating it effectively to clinicians or managers in the NHS in order to help them effect change. There is also an important role for individuals working as analysts, using more ‚Äòpoint and click‚Äô interactive tools developed for them by others who have deep skills in data management and analysis\u0026rdquo;\nIn respect of workforce:\n It is, however, crucial [emphasis added] that a large number of people have basic skills in this space, where they are developing and implementing analyses\n In respect of culture and practice:\n it is crucial that [emphasis added] the core working practices, such as sharing code, are implemented as a norm throughout the system, because of the problems that closed working can create around quality, safety, usability, credibility, and review\n Point and click #   There is a legitimate and widespread ambition to have point and click tools for analytics in the NHS\u0026hellip; However, it can only be realised by teams that understand the true underpinning reality of how patient data is generated, extracted, prepared and used across the system; who have RAP and computational skills; and who can work alongside people with clinical analytic needs to iteratively develop prototypes of interactive tools; and can then work with other tools to harden the most suitable of those prototypes into scalable interactive tools. Any solution that appears to not entail this kind of process, and workforce, is simply hiding the work, by commissioning it in less effective and closed means, or similar activity. By doing so, the system is prevented from learning about its own data, and developing the open commons of knowledge and workforce that drives high quality analytics and research in other settings and fields\n Build versus buy #   Overall, as discussed in the sections below, the best approach is: re-use existing commercial or open tools that already exist for large tasks (databases, etc) according to which is the best, with a preference for open to ensure access across hundreds of organisations in the NHS; procure open code from public and private vendors for new substantial tasks; build in-house and procure open code from public and private vendors for the vast ongoing workload of data management and analysis within those larger tools.\n Barriers to RAP #  Skills #  \u0026ldquo;there is an almost limitless array of self-directed online teaching through services such as Coursera, or some MOOCs (Massive Open Online Courses), but no clear signposting or curation of ‚Äòjourneys‚Äô through these courses, or guidance on which to choose\u0026rdquo;\n\u0026ldquo;For NHS service analysts\u0026hellip; there is less access to formal training. The recent development of some online discussion forums aiming to drive further training was welcomed, but at present this does seem somewhat limited to links out to various YouTube videos on analysis in adjacent analytic fields, without clear curation of quality, appropriateness, priorities or journeys\u0026rdquo;\nRecruitment #  Recruitment and retention are very difficult in universities and NHS organisations because of the relatively low status and salary that these jobs have compared with equivalents in the private sector.\n\u0026ldquo;it is uncommon to find experienced software developers with industry standard skills who also have strong domain knowledge across topics such as: the nature of NHS data; how research is done with electronic health records or related research data; the clinical context for such work; the operational context of the NHS; how data is collected, manipulated, and extracted from clinical systems; and so on. This is particularly concerning given that senior individuals in the NHS and academia repeatedly expressed frustration that they wanted to recruit these staff, but that this is \u0026lsquo;like hunting unicorns\u0026rsquo;\u0026rdquo;\n\u0026ldquo;while some developers will accept a lower salary for public service, many equate this ‚Äòpublic good‚Äô with contributing to open source sharing of code, whereas in academia and the NHS code is often closed. Related to this, developers typically use evidence of prior work to get each new job, and often use their GitHub activity to help future employers see their productivity, and quality; this is not possible when code is withheld from open view; when projects are slow to deliver; or where the software contributions are low status or hidden\u0026rdquo;\nPlatforms #  \u0026ldquo;those with skills in open approaches to computational data science\u0026hellip; were actively blocked by the platforms and tools available to them in both the NHS and academia. On a small scale, at the level of individual laptops, they were often concerned to find that local IT policies actively prohibited the installation of tools ‚Äì such as R, Git, Python or Docker ‚Äì which they viewed as being a basic minimum necessity for the delivery of their work. Where it was possible to push through these obstructions, it took very substantial effort on the part of individuals, often requiring substantial local organisational and administrative support from senior leaders, over a very long period of time\u0026rdquo;\n\u0026ldquo;these problems also seem to be prevalent in many of the large and small platforms that have been created for secure analysis of data. For example, numerous analysts expressed concern and surprise that they weren‚Äôt able to use GitHub inside Trusted Research Environments, or Data Access Environments, because communication with outside resources such as these were locked down for security reasons; and furthermore, that they were not even able to access more closed tools for code management such as GitLab, as they were either unavailable or implemented so poorly as to obstruct their normal use\u0026rdquo;\n\u0026ldquo;in a technical environment where all the tooling, working practices and assumptions are built on a model substantially less evolved than RAP, procurement and implementation decisions are built around an assumption of people using point-and-click and copy-and-paste methods, rather than scripts, meaning that individuals with computational skills simply cannot use them\u0026rdquo;\nFunding #  \u0026ldquo;Numerous individuals complained that they were able to find no open competitive or conventional sources of funding to support them as individuals, or as a team, to focus on the software and related methodological aspects of delivering high quality outputs from NHS data, and that their salaries were therefore only covered as a component part of a grant focused on delivering a specific research paper output, adding to their sense of lower status, and obstructions in both their career and their ability to innovate, as they were only employed, conceived of, tasked, and supervised as ‚Äòsupport staff‚Äô for traditional academic epidemiology research skills. In desk research it proved extremely difficult to find open competitive sources of funding for this kind of work from any national funders, whether project-based or person-based.\nMore concerningly, a number of individuals described in detail situations where a substantial public investment had been made to deliver work closer to research data infrastructure, code, and methods, but that this resource had been ‚Äì in their view ‚Äì diverted onto delivery of traditional research outputs, and staff with only skills to deliver those outputs, either because the specific piece of funding had been administered and awarded from funders to individuals with traditional research paper skills, rather than those with computational skills, or because those in senior leadership and strategic roles in their organisations tended to be those with a traditional focus on single research paper outputs rather than code.\u0026rdquo;\nRecognition #  \u0026ldquo;Researchers and analysts with strong skills in RAP and computational approaches to data science expressed a range of frustrations that clearly reflect fixable structural challenges. At an individual and organisational level, there was an impression that these skills are under-valued, or that they were viewed as ‚Äòjust programmers‚Äô. In academia examples were shared of developers and data scientists not being named as authors on publications, or grants, despite having contributed ‚Äì in their view ‚Äì a very substantial amount of the work, including creative and innovative methodological work to deliver the outputs. Examples were also shared of situations where individuals had left the field and moved into other industries where they could get both higher salaries and better recognition for their skills.\u0026rdquo;\nRecommendations #  [mostly verbatim, edits for length]\nOpen 1. Create a RAP and Open Code Oversight Group\nOpen 2. Create a public policy setting out expectations on open code\nOpen 3. Make open code a boilerplate feature of all public contracts\nOpen code sharing should be a required feature of all standard contracts between the NHS and any external provider of code for health data management and analysis [emphasis added]\nOpen 4. Create an ‚Äòexceptions framework‚Äô whereby publicly funded code can be closed by prior arrangement if this meets NHS and UKplc strategic objectives\nIndividual researchers, organisations or funders should be able to actively apply for a single project to be closed, under a carefully designed ‚Äòexceptions‚Äô framework, where each exceptional request is evaluated to determine whether this exception meets reasonable national, individual or organisational interests around commercialisation and collaboration, and whether the network damage inflicted by a closed license is justified by another greater public benefit. This expert group should include intellectual property specialists, software developers, researchers, key stakeholders with expertise (such as the Open Data Institute), policy experts and funders.\nOpen 5. Create an Open Code Ombudsman and Assistance Unit\nIt is possible, particularly whilst the NHS analytics workforce is in a transition period, that there will be occasions when an analyst or team of analysts in one NHS organisation needs access to code that has not yet been made open by a different NHS organisation ‚Äì potentially including central government organisations. There is a need for an independent body that is tasked with dealing with these, and other, disputes. This unit should listen to complaints, and feedback common themes to the relevant policy, funding and commissioning teams so that appropriate guidance can be developed. To make this practical, the unit must have appropriate status and power over even the largest NHS organisations.\nOpen 6. Assert that publicly funded code is publicly owned: cautiously consider ‚ÄòCrown Copyright for code‚Äô\nAt present decisions about sharing and licensing code are made, at small scale, in huge numbers, across the health and research landscape, often by people who do not understand the impact and implications of these choices for themselves and/or the wider community of data users. This has very substantially blocked code sharing, which should be the norm, and is the norm in many adjacent research specialities. An expert group should be convened to formally consider a new national standard: that public funded code is publicly owned, under a formal license that covers all code produced on public funds; with an expectation that all publicly funded code should be shared under the MIT open license; and exceptions to be decided by prior arrangement with a prespecified ruleset.\nOpen 7. Data Controllers should require RAP and open code sharing from data users\nData controllers and especially the NHS should require all those accessing patients‚Äô data to share all analysis code, or openly argue for exceptions in single cases. Where the system is in a period of transition, this should be strongly considered in all data access requests, and where there is no plan to share code immediately there should be a credible plan to ensure that this is done later. As with other mandates around open code this should have a clear pre-specified exceptions framework.\nOpen 8. Amend the Code of Practice for the Research Powers of the Digital Economy Act\nThe Department of Health and Social Care and the UK Health Security Agency should work with the Department of Digital, Culture, Media and Sport to make a minor amendment to the Code of Practice for the Research Powers of the Digital Economy Act (the primary legal gateway for accessing non-health data) requiring data analysts to make their code available, with a robust exceptions framework as discussed elsewhere.\nOpen 9. Make it ‚Äòokay to ask‚Äô about access to publicly funded code\nThe team heard from several interviewees that at present it is commonly regarded as provocative to ask for access to the code used to implement an analysis, especially in some parts of the academic community, despite general positive statements on open working. Culture change will only be possible if it is deemed socially acceptable to question when deviations from the ‚Äònew norm‚Äô of open working are identified. This should be made clear in all relevant policies, and codes of conduct, across academic and NHS organisations.\nOpen 10. Health and Care Information Governance Panel guidance should facilitate open code\nIt is important that all NHS organisations are given clear direction that code sharing is not the same as data sharing and that it is entirely possible to share code routinely and safely without the organisation incurring significant costs or reputation risks. To make this clear, the Health and Care Information Governance Panel should create guidance on the importance (and permissibility) of code sharing, to go on the Information Governance Portal, emphasising that transparency is a crucial means to build public trust and clinical safety.\nOpen 11. The Information Commissioner‚Äôs Office should produce guidance to facilitate code sharing\nThe Information Commissioner‚Äôs Office (ICO) is currently working to produce new and updated guidance on anonymisation. Alongside this, the ICO should also produce guidance regarding code sharing. This should make clear that sharing code is not a disclosure risk, and that those writing code have a clear responsibility to ensure that their analytic code is not disclosive of any personal information. Ideally this guidance should also make clear that code sharing and the practices associated with Reproducible Analytical Pipelines are an important aspect of good citizenship around data usage. There is also a need for better guidance and training on ensuring that analytic code is not disclosive of any personal information.\nOpen 12. The Medicines and Healthcare products Regulatory Agency should address code sharing and device regulation\nOpen 13. Negotiate co-ownership of claimed commercial innovations from NHS data\nOpen 14. Write an ‚Äòopen analytics policy for the NHS‚Äô\nBring together DHSC and the NHS Transformation Directorate to write a policy that makes it clear to all analyst teams across the NHS, and all general managers, that sharing code is not the same as sharing data and that open is the preferred and default method for all analysis conducted using public data and public funding\nThis policy should set out best practice for using open working methods, for openly sharing code and for writing documentation. It should also cover more complex areas such as licensing and the protection of IP where applicable. It should be kept under regular review to ensure it remains up-to-date and should signpost to further sources of help and advice where necessary. All external procurement of data science services, whether data management or analysis, should require that all code and codelists produced to deliver the work are shared openly by default. Exceptions to this should be rare and explicitly pre-arranged, with clear justification under pre-specified criteria set by the NHS Transformation Directorate and DHSC\nOpen 15. Make open a standard contractual requirement\nGovernment departments should require all those conducting data analysis on their behalf, in-house and as contractors, to share all code, consistent with RAP and the computational methods above, with adequate technical documentation as per RAP criteria. To aid with this Intellectual Property assignment and publication requirements should be laid out in template and framework contracts so that all organisations commissioning or contributing analysis and code to the public sector are held to the same standards. These contractual requirements should be developed in collaboration with members of the research software engineering community to ensure that they are fit for purpose. It should also be borne in mind that all such requirements, policies, and guidance documents are likely to be ‚Äòliving‚Äô and regularly iterated best on the evolving nature of best practice in this field\nOpen 16. Commission intermittent open code audits to drive improvement\nIn collaboration with academics and key organisations such as AphA and the NHS-R community, the NHS Transformation Directorate should commission regular code audits of all organisations that have received public funding for health data research or analysis, including funding for the development of intermediate knowledge objects (for example, datasets, or TREs). These audits should evaluate adherence to RAP and open computational methods; follow a set methodology; be published openly; and be used for the explicit purpose of improving performance on code sharing, rather than penalising poor performance.\nSpecific criteria should be developed in collaboration with the community but include: all code shared on GitHub or similar; adequate technical documentation embedded within code as per RAP criteria; use of version control and appropriate methods; sharing of non-disclosive open data where possible; support for continuing professional development in work time; whether staff meet job descriptons with training, continuing professional development, or other proof.\nOpen 17. Establish a technical writing and documentation team for the NHS\nOpen 18. Create a ‚ÄòCode For Health‚Äô training programme for NHS service analysts and academic researchers\nThis should include advanced computational data science covering RAP, software carpentry, version control, functions, code documentation, and similar. This should be at a range of levels including MOOCs, short courses, and long courses, with strong practical elements. The principles and working practices set out in the box below are strongly recommended.\n  Combine NHS and academia\n Although historically NHS service analysts and academic analysts are considered separately, this is a strong strategic opportunity to begin building robust technical bridges between the two: both work on similar data, often with similar methods or tools; and both should ideally work in similar data analysis environments, as discussed in the Trusted and Shared Research Environments chapter.    MOOCs and practical work\n Traditional teaching and training has a role. However, for scope and access, a priority should be placed on delivering training as a combination of Massive Open Online Courses (MOOCs) and in-person teaching, both developed specifically for this purpose. MOOCs can cover factual content, and with practical code development exercises can be procured outright: they should be made openly available to all. In-person supervision is necessary for a subset of attenders, especially those seeking certification, to supervise practical work, and for marking work to evaluate competences. This will necessitate a per-person fee, which will create a revenue stream for providers, but necessitate a system for unit payment from the NHS which may obstruct NHS analyst attenders given current lower status of this group in some organisations (see NHS Analysts chapter): thought should be given to block purchase or training budgets as seen in other parts of the NHS.    Build on prior work but maintain focus on RAP\n This training should build appropriately on various existing resources and expertise including: the work by existing RAP teams; the work by the NHS-R Community; and other work specifically on RAP and computational methods. It should focus specifically on RAP and computational data science techniques as directly applied to working with NHS data. Caution should be taken that resource is not diverted into training on other issues such as general research methods, specific statistical methods (except as specifically embodied of RAP training), or newer methods such as Machine Learning which are useful but different subjects. Similarly this training cannot be delivered by universities rebadging existing training on other topics such as bioinformatics; or by simply linking out to generic data science training resources from other suppliers; albeit that these may be fertile starting points for modification into bespoke training on RAP and computational methods in NHS data.    Open competitive procurement\n Training should be procured by an open competitive process, amenable to the best of either public or private providers. All training can be commissioned by either UKRI, the NHS, or both. A rapid technical review should be conducted of recent very welcome UKRI investment in this space to evaluate whether it addresses RAP and computational methods as above, and whether outputs are open. There is currently a rich diverse ecosystem of training in many other aspects of analytics, with no concern about overlap between offers, and similarly a diverse range of providers with different focuses should be acceptable here. An emphasis should be placed on finding providers with strong previous track record of using RAP or open computational methods.    Open 19. Create a ‚ÄòData for NHS leaders‚Äô training programme\nFor senior leaders and those in adjacent skill groups training should be accessible on the basics of data analysis, RAP and computational methods so that this group can understand the principles and purpose of the work done by those in their team. This should be MOOCs and short courses. Some of the excellent recent work at the Number 10 Data Science Unit (10ds) ‚Äòdata for leaders‚Äô programme may be adaptable.\nOpen 20. Create an ‚ÄòNHS data for developers and data scientists‚Äô training programme\nVery senior leaders in the system repeatedly expressed to us the view that individuals with deep developer skills and NHS domain knowledge are extremely hard to recruit, if not impossible. Strong software development and data science skills are developed over many years and often require a specific disposition: it is not realistic to expect that current NHS analysts (with a deep but very different knowledge base) can be trained in such skills to the standard required by the NHS and wider community. New knowledge for generalist Software Developers and Data Scientists should be addressed with health data ‚ÄòTransfer Training‚Äô so that individuals with strong technical skills can develop deep NHS domain knowledge.\nOpen 21. Modify the Research Excellence Framework (REF) to reflect computational work\nOpen 23. Pay realistic salaries to software developers\nSoftware developers are among the highest paid staff globally, but university pay scales typically do not recognise the skillsets required to be a research software engineer, and often attempt to hire engineers and developers on pay-grades similar to those of IT support staff. This makes it hard to recruit people with outstanding software skills and serves to further undermine the value of software development, data management, data curation, and code development. It is commonplace for universities to pay those with technical skills, such as clinicians or accountants, something closer to their realistic market salaries, Universities should develop pay scales for developers in the same way that they have done for clinical academics, recognising the specialist skills and outside options of those they are seeing to recruit.\nOpen 24. Create a working group to develop an attribution model for code and data\nOpen 25. Clarify the need for authorship for software developers and data scientists as equal core contributors\n\u0026hellip;Overall, having discussed this issue extensively, it is clear that there should be a presumption to include software developers and data scientists who have contributed to the delivery of [papers] in authorship, not least because this work commonly entails a wide range of creative input to deliver the work informed by deep technical knowledge of the analysis, but also in very many cases the clinical domain, and the statistical context, and similar issues. Where this is argued to conflict with other current documentation on principles such as guidance from the International Committee of Medical Journal Editors ‚Äì which is itself commonly breached across a range of other topics ‚Äì this should be robustly addressed and discussed\u0026hellip;\nOpen 26. Proactively address sharing during the pandemic\nAcademic researchers have played an essential role in the response to the global COVID-19 pandemic. Yet, despite the global nature of the emergency, not all research, code, and other outputs have been made openly available for others to re-use or learn from. In some instances there may be good reasons for this, but in others it may simply be that the barriers to sharing have proved too high for some research groups ‚Äì for example, paying for GitHub or for open-access publication. University administration teams and innovation offices should discuss with researchers providing research outputs on COVID-19 whether they can share code, methods, documentation, libraries, or more, for recent and future outputs.\nOpen 27. Academic journals should be encouraged to make code-sharing a requirement\nOpen 28. Embrace research software engineering with 3 data pioneer groups leading by example\nOpen 31. Provide guidance and training on RAP and code sharing\nThe training above covers broader issues around RAP and code sharing. Some researchers currently funded by for example NIHR and UKRI may be willing to share code but require bespoke training and support to do so. To help these individuals with this transition, funders should provide guidance and funding to attend training on ‚Äògood enough‚Äô code sharing, alongside guidance and examples of ‚Äòperfect world‚Äô code sharing. This should include supporting existing work, such as the Turing Way, RAP, and other similar projects.\nOpen 32. Fund a fellowship programme around Code for Health Data.\nThere is currently a need for more teams and individuals who combine skills in computational methods, alongside domain knowledge around epidemiology, NHS analytics, and EHR data. This can be addressed by applying the normal mechanisms for capacity building, especially fellowships which bring independent status within the university sector for individuals and a field; support career progression; and support individuals to make their own choices about where they can best contribute and innovate collaboratively alongside pure research colleagues, to get away from a paradigm of developers being seen as ‚Äòinstructed by‚Äô researchers.\nOpen 33. Open funding calls for projects and programmes around Code for Health Data.\nWork related to technical infrastructure, data management, and TREs is not universally regarded as legitimate academic or methodological activity. There are multiple sources of funding for Individuals and teams to address specific single clinical research questions, but almost no open competitive funding for the development of code, infrastructure, or innovative methods in this space. It is crucial that great code and data infrastructure is developed in close collaboration with the delivery of strong single academic research paper outputs. However strong code is not produced when this aspect of the work is left to fend for itself as a junior party to single topic research projects, especially during a period of transition towards more computational approaches. UKRI and/or NIHR should launch an open funding call specifically for open code projects in health data science, and consult closely with the Wellcome Data for Science group on the best means to achieve this.\nOpen 34. Treat ‚Äòdata Infrastructure‚Äô as open code\nRecent work from UKRI increasingly recognises that modern infrastructure is less about computers and buildings, and more about code and teams. It is crucial that data infrastructure is handled in this way, with delivery of open code at its core, accompanied by detailed technical documentation alongside it, consistent with RAP and the computational approaches outlined above. It is equally crucial to recognise that this approach can and should viewed as modular (with different re-usable elements for different tasks produced by different teams) and methodological (with approaches to specific data management, data analysis, and privacy preservation tasks developed iteratively through innovation and delivery). Most importantly, the system must steer carefully away from procuring infrastructure such as TREs as ‚Äòblack box services‚Äô where only comms material and finished academic manuscripts can be seen from the outside; this is covered more in the chapter on Trusted Research Environments\nOpen 35. Use open competitive funding for code projects\nIt is vital to support an open collaborative ecosystem where those with the best ideas and delivery can compete to propose the best approaches to diverse challenges in data management, analysis, analytic platform components, and so on. An approach where one organisation, or a small number of organisations, is viewed as the sole supplier will not lead to excellence. Open competitive funding, where all can propose projects, is a key route to ensuring we identify and resource the best ideas and teams.\nOpen 36. Review prior delivery of open code by applicants\nDuring a period of transition to new ways of working it is important that those leading by example are enabled to drive change; and prior delivery in this comparatively new space will likely be a strong predictor of future delivery. Particular caution should be used around teams with prior substantial resources for health data research projects that have not delivered, or cannot retrospectively showcase, a commensurate set of documented code.\nOpen 37. Ensure experts on code select and oversee code projects\nIndividuals with direct technical expertise in software projects, data infrastructure, RSE, RAP and related work should lead or very strongly guide all funding for projects in this space, just as conventional academics guide awards and oversight for projects delivering single academic paper outputs.\nOpen 38. Ensure the objectives and outputs of all investments are open\nAll funding for code projects, especially those around infrastructure, should be openly and publicly disclosed, with a brief description of the amount, the recipient, the expected work and timelines, and a link to the repositories where development and documentation is anticipated to reside.\nOpen 39. Ensure funding for code and platforms does not get diverted\nWhen providing bespoke funding for developing open code and appropriate technical platforms it is necessary to ensure resource is not diverted to fund single research paper outputs for which there is extensive funding through other routes. This will require oversight at a number of different levels. When applications are being reviewed, funders should make sure that they have relevant technical experts on the panel reviewing the applications ‚Äì for example, research software engineers and data scientists ‚Äì evaluating proof of skills such as GitHub profiles and repositories in addition to CVs.\nOpen 40. Avoid ‚Äòregressive funding models‚Äô built around short-term bursts of funding\nThere is a concerning tendency for code projects to be resourced with short-term urgent bursts (for the avoidance of doubt, outside of COVID-19) with funding arrangements that require successful applicants in extremis to spend a large amount of money very suddenly, over less than a year, starting with only a few weeks‚Äô notice. This strongly suggests a lack of strategic thinking in the organisations offering funding, and will tend to preferentially resource large incumbents who can rapidly absorb such costs, but who may not have the best prospects of delivering for the wider community. More specifically this last-minute funding strategy actively mitigates against new entrants to the market, field, and creative academic pool, while favouring incumbents; and disadvantages those in more junior roles, or without large existing teams, who may have the strongest ideas, newest skills, and best delivery prospects. The preferred funding approach should operate on 2 to 5 year cycles to build capacity and open delivery, with the conventional option of 6 months from award to commencement to facilitate inward recruitment of staff.\nOpen 41. Focus on sustainability for software projects: set aside a third of resource for this task\nFunding should work hard to identify teams and projects with broad user bases and impact, then support them to continue their work with 2 to 5 year funding. This should not entail unrealistic proposals for commercialisation of data management and analysis code used solely for research and health data analytics unless there are clear grounds for a standalone commercial spinout; and it should not necessarily entail extensive commitments to specific new features simply to justify sustaining a project. Iterative improvement and sustained delivery are sufficient goals in themselves, build capacity in the system, and build a broader ecosystem of productive outputs. A third of any budget should be set aside for sustainability.\nOpen 42. All Trusted Research Environments for NHS data must facilitate and require code sharing\nOpen 43. TREs themselves should be built on principles of RAP and open code\nOpen 44. Produce clear guidance on disclosure risk and open code\n"},{"id":13,"href":"/notebook/docs/analyst-skills/team_types/","title":"Team Types","section":"Analyst skills","content":"Team types #  Summary #  Broadly, we might think of three types of analytic team within the ICS:\n Performance Data analysis/ reporting Data science Public health/ population health  The roles of these teams is discussed in more detail following. Quoted content comes from the data job family which is licensed under the OGL3.\nPerformance #  Performance teams:\n ‚Ä¶develop performance measurement frameworks - key performance indicators (KPIs), goals, user needs and benefits - and analyse the performance of a service or product against these, adapting your approach and framework appropriately and in line with any changes\n Tools and approaches\n SPC RAG ratings Board level summaries of KPIs  Data analysis and reporting #  Data analysts:\n ‚Äúapply tools and techniques for data analysis and data visualisation (including the use of business information tools), collect and migrate data to and from a range of systems manage, clean, abstract and aggregate data alongside a range of analytical studies on that data manipulate and link different data sets summarise and present data and conclusions in the most appropriate format for users‚Äù\n Tools and approaches\n PowerBI/ Qlik/ Tableau Benchmarking SPC Statistical analysis SQL  Data science #  Data scientists\n \u0026hellip;use data to identify and solve complex business problems. They have an interdisciplinary focus, using techniques and knowledge from a range of scientific and computer science disciplines (for example, statistics, analytics, machine learning)\n Tools and approaches\n R/ Python/ Julia/ SQL Time series analysis/ forecasting Statistics (especially e.g. regression models, GAMs, GEEs, etc.) Machine learning Natural language processing Open tooling/ version control  Public health/ population health #  Population Health Management:\n \u0026hellip;improves population health by data driven planning and delivery of proactive care to achieve maximum impact. It includes segmentation, stratification and impactabilty modelling to identify local ‚Äòat risk‚Äô cohorts - and, in turn, designing and targeting interventions to prevent ill-health and to improve care and support for people with ongoing health conditions and reducing unwarranted variations in outcomes\n Tools and approaches\n Segmentation Health economics Public health and epidemiological analysis Impactability modelling Population health profiling Opportunity analysis  "},{"id":14,"href":"/notebook/docs/datascience/copyright/","title":"Copyright","section":"Data science in the NHS","content":"Copyright #  Intellectual property #  Intellectual property is an umbrella term which principally refers to copyright, trademarking, design rights, and patents. Trademarks and design rights, although they can sometimes be relevant in the world of software development (for example, in the story of the Debian linux distribution rebranding Firefox to Iceweasel) are only obliquely related to the present matters of discussion and will not be explored any further here.\nUnder the Berne convention, copyright is assigned automatically to any expression (such as a piece of music, a book, an artwork, or a set of computer code). It is not necessary to apply for copyright. Where a literary, dramatic, musical or artistic work, or a film, is made by an employee in the course of her employment (\u0026ldquo;work for hire\u0026rdquo;), her employer is the first owner of any copyright in the work (unless they agree otherwise). The expression \u0026ldquo;in the course of employment\u0026rdquo; is not well defined but the crucial difference is between \u0026ldquo;contract of service\u0026rdquo; (eg as an employee) and \u0026ldquo;contract for services\u0026rdquo; (eg as a freelancer). Individuals working under a contract of service will usually maintain copyright (unless they agree otherwise).\nPatents are a very different thing, and give intellectual property over an idea. Some argue that software patents should not exist at all and there is much debate about where the boundaries should be between ideas in software that can and cannot be patented (for example in the case of non-obviousness). The ideas and debates in this area are very important and interesting in the world of software but given the reservations many free software proponents have about software patents (and the complexity and expense of acquiring and defending software patents) they are clearly not a suitable thing for NHS data science teams to acquire and will not be discussed further here.\nCopyright, on the other hand, cannot be avoided since, as discussed above, it is automatically attributed at the moment of creation to the individual creator(s) or their employing organisation. Failing to discuss this issue at the beginning of a project could lead to confusion and disagreement once a project has begun or been completed. In a sense the holder of copyright for a piece of software licensed as FOSS (whether permissively or as copyleft) is unimportant, since the software licence ensures that anybody can use the code for any purpose (subject to the restrictions of copyleft where this is applied). However, one important power that a copyright holder has on a piece of open source software is the ability to relicense. Relicensing takes place when copyright holders wish to take existing code and place it under another licence. There is a great deal of confusion about exactly how to ensure that code can be relicensed legally, but there are examples of where this has been done successfully here, here, and here. In short, it is widely believed that in order to relicense computer code it is necessary to get the approval of all, or nearly all, of the contributors. Some projects, such as the Linux kernel, have thousands of contributors and are therefore widely seen as impossible to relicense. Where only one individual or organisation holds the copyright relicensing is simple. The FSF ensures that the copyright of all of their software projects is explicitly granted to them by all contributors so they can responsibly exercise the powers that being a copyright holder grants.\nCrown copyright #  The data scientists at the Government Digital Service have no need to concern themselves with copyright in their day to day work since\n All code produced by civil servants is automatically covered by Crown Copyright\n Source.\nThe NHS, by contrast, because is a patchwork of national and local bodies (NHS England, Nottingham City CCG, Nottinghamshire Healthcare NHS Trust\u0026hellip;), can potentially produce code that is copyright to any one of hundreds of different organisations. When these bodies cooperate they could produce code licensed to a confusing patchwork of different organisations. Individuals working for the NHS should be able to cooperate on open source projects without engaging in lengthy negotiations about which organisations will hold copyright and, furthermore, it is desirable that individuals in other NHS organisations can freely contribute without worrying about whether their employer expects to see their \u0026ldquo;share\u0026rdquo; of the intellectual property. A solution to this problem would be for there to be a presumption that NHS staff releasing open source code do so under a Crown copyright licence, unless there is a reason why an exception to this rule should be made. There is precedent for assigning copyright to the crown but assigning it routinely to open source code produced by NHS staff has not been tried previously.\n"},{"id":15,"href":"/notebook/docs/goldacre_review/","title":"Goldacre Review","section":"Docs","content":"Better, Broader, Safer: Using Health Data for Research and Analysis #  These are my notes from the material in the Goldacare Review. Please note that if you\u0026rsquo;re not me and you\u0026rsquo;re reading this I am only making notes about the bits that particularly pertain to me as a data scientist working in an NHS provider trust. For example, I think trusted research environments are a great idea, but it\u0026rsquo;s not of particularly great interest to me in terms of my work, so I\u0026rsquo;m skimming over those bits. I\u0026rsquo;m interested in how we use data at the coalface from within an organisation (or ICS, IG permitting) and in particular how we use data to inform decision making and ensure that staff and patients are getting the best outcomes possible (clinical and experiential).\nNote that the material is provided under the OGL version 3, and I am complying with the licence by reusing the material with proper attribution and a link to the licence. I read a lot of stuff, code and prose, that is produced with public money without an open licence so I would like to take this opportunity to thank the authors and publishers for opening the material in this way.\nTerms of reference #  [verbatim from document]\n How do we facilitate access to NHS data by researchers, commissioners, and innovators, while preserving patient privacy? What types of technical platforms, Trusted Research Environments (TREs), and data flows are the most efficient, and safe, for which common analytic tasks? How do we overcome the technical and cultural barriers to achieving this goal, and how can they be rapidly overcome? Where (with appropriate sensitivity) have current approaches been successful, and where have they struggled? How do we avoid unhelpful monopolies being asserted over data access for analysis? What are the right responsibilities and expectations on open and transparent sharing of data and code for arm‚Äôs length bodies, clinicians, researchers, research funders, electronic health records and other software vendors, providers of medical services, and innovators? And how do we ensure these are met? How can we best incentivise and resource practically useful data science by the public and private sectors? What roles must the state perform, and which are best delivered through a mixed economy? How can we ensure true delivery is rewarded? How significantly do the issues of data quality, completeness, and harmonisation across the system affect the range of research uses of the data available from health and social care? Given the current quality issues, what research is the UK optimally placed to support now, and what changes would be needed to optimise our position in the next 3 years? If data is made available for secondary research, for example to a company developing new treatments, then how can we prove to patients that privacy is preserved, beyond simple reassurance? How can data curation best be delivered, cost effectively, to meet these researchers‚Äô needs? We will ensure alignment with Science Research and Evidence (SRE) research priorities and Office for Life Sciences (OLS) (including the data curation programme bid). What can we take from the successes and best practice in data science, commercial, and open source software development communities? How do we help the NHS to analyse and use data routinely to improve quality, safety and efficiency?  Summary #  Key principles #  Good defaults- make good practice the norm, not the exception #   \u0026ldquo;Create an ‚Äòexceptions framework‚Äô whereby publicly funded code can be closed [only by] by prior arrangement if this meets NHS and UKplc strategic objectives\u0026rdquo; \u0026ldquo;it is crucial that [emphasis added] the core working practices [of RAP], such as sharing code, are implemented as a norm throughout the system, because of the problems that closed working can create around quality, safety, usability, credibility, and review\u0026rdquo;  Workforce #   \u0026ldquo;Point and click tools have their place, but \u0026ldquo;[they] can only be [produced] by teams that understand the true underpinning reality of how patient data is generated, extracted, prepared and used across the system; who have RAP and computational skills\u0026hellip; Any solution that appears to not entail this kind of process, and workforce, is simply hiding the work, by commissioning it in less effective and closed means, or similar activity\u0026rdquo; \u0026ldquo;there is an almost limitless array of self-directed online teaching through services such as Coursera, or some MOOCs (Massive Open Online Courses), but no clear signposting or curation of ‚Äòjourneys‚Äô through these courses, or guidance on which to choose\u0026rdquo;  Modernising NHS service analytics #   WIP  Modern, open working methods for NHS data analysis #   Open 4. Create an ‚Äòexceptions framework‚Äô whereby publicly funded code can be closed by prior arrangement if this meets NHS and UKplc strategic objectives  Note that good practice around code sharing is good practice around coding. Good code should never include any personal information, whether it\u0026rsquo;s being shared or not.\nThe challenge of privacy in health data #  Trusted Research Environments #  Information governance, ethics and participation #  Data curation #  Strategy #  "},{"id":16,"href":"/notebook/docs/goldacre_review/privacy/","title":"Privacy","section":"Goldacre Review","content":"The challenge of privacy in health data #  High quality analysis requires very detailed information about individuals, which means that a group of analysts have \u0026ldquo;access to every recorded detail, of every medical event, for almost every citizen, all the way back to birth\u0026rdquo; (I\u0026rsquo;m a little dubious of this claim myself. I\u0026rsquo;m not sure anyone could give you this even for the patients in my Trust, limiting themselves only to events within the Trust).\n Managing this problem ‚Äì widening access to records, while also preserving patients‚Äô privacy ‚Äì is the fundamental challenge for use of NHS data in service improvement, academic research, and the life sciences sector\n  The NHS must maintain trust and active enthusiasm from patients and the public Researchers and analysts, conversely, are deeply frustrated by inaccessibility of data, and missed opportunities to improve patient care, when slow information governance processes obstruct data access  Current practice #  Privacy is currently maintained in two ways: pseudonymisation and trust in individuals and organisations.\nPseudonymisation refers to removing individual identifiers such as DOB and postcode. In practice with a large dataset it is very vulnerable to the reidentification of individuals.\n Knowing the approximate date range in which someone had a medical intervention, their approximate age, and their approximate location is often enough to re-identify someone in a pseudonymised dataset, and then ‚Äì illegally ‚Äì to see everything else in their record\n Mothers are particularly vulnerable since their approximate age and when they gave birth can be inferred by many individuals known to them. The more individuals in the dataset, and the more detailed the dataset, the greater the risk of reidentification.\nBecause pseudonymisation is imperfect in practice the system relies also on putting trust in a limited number of individuals and organisations. This creates a large bureaucratic overhead and will intrinsically not scale.\nIt is worth noting finally that the current systems have failed in their attempt to reassure the public about the use of their data.\nNHS data is not confined to the national datasets (e.g. at NHS Digital) that tend to be the focus of discussions. Because of a lack of clear guidance and secure analytics platforms, every GP practice and NHS Trust is an independent data controller. This leads to a large, confusing patchwork of data flows which are often not consistent even regionally, and certainly are ill defined at a national level.\n National GP data flowing into a Trusted Research Environment (TRE- see below and the section on TRE) is therefore an important privacy safeguard for patients, a substantial net improvement in protections for patients, and a reduction in burden around data flows for GPs\n The consequences of lack of trust in privacy #  Because of a lack of trust in large data collection programmes such as care.data and GPDPR there are now many individuals opting out of the programme, \u0026ldquo;at a scale that will compromise the usefulness of the data\u0026rdquo;\nTrusted research environments #   \u0026hellip;there is a clear path forwards. In many other sectors ‚Äì such as census work at ONS for 2 decades ‚Äì data is not disseminated out to users. Instead the analysts go to the data, and work inside a secure platform called a Trusted Research Environment (TRE). This working style must [emphasis added] be adopted in the NHS\n There should be no expansion of the use of pseudonymised data until TREs are in place- worth noting that the GPDPR dataset will only be available within a TRE.\nOther methods #  \u0026ldquo;There are other forms of risk mitigation including: removal of ‚Äòsensitive codes‚Äô (which obstructs research on key areas of medicine); data minimisation (which has uses but is under-researched); sub-sampling (which has limits when aiming to detect subtle statistical signals); data perturbation (which has a role but requires a substantial research programme, and is complex to implement); and emergent methods such as ‚Äòhomomorphic encryption‚Äô (which has seen no substantial working health implementation to date). Overall they show that this an important area of work which has been relatively neglected. Wider access to NHS patient records requires that the system as a whole takes the challenge of practical approaches to secure analytics, developing and evaluating robust methods for protecting patients privacy at scale.\nThere is a clear role for UKRI/NIHR in providing open, competitive resource for applied methods research into privacy preservation, to earn public trust, in collaboration across the NHS, epidemiology and security engineering communities\u0026rdquo;\n[note that there is a substantial amount of discussion in the paper about different privacy protecting methods as well as reidentification risks, these have been omitted in the interests of brevity, but if you\u0026rsquo;re interested they are very much worth reading]\nConflict between the views of the public and researchers #  The reviewers heard from two groups:\n campaigners, patients, policymakers, and professionals about the need for patients‚Äô privacy to be respected, about the need for medical records to be handled confidentially, and the substantial shortcomings in current ways of working with health data, such as disseminating detailed data out to multiple locations where subsequent activity is less clearly monitored researchers, funders, NHS analysts and senior leaders in the health service and government, [who said that] that the administrative, regulatory, practical and legal aspects of information governance (IG) are obstructive, duplicative, expensive, slow, inefficient, confusing, inconsistent and disproportionate to the risks presented by their work  It was been widely argued that these two views are inherently opposed and lead to a simple trade off between the two. The report authors argue that in fact it is possible to achieve both aims at the same time using the methods detailed in the report.\n The set-up of siloed, and slightly out-of-date data was fine for 25 years ago. This is no longer a viable model. We can have multiple eyes on the data in near real-time\n \u0026ldquo;\u0026hellip;all pseudonymised national detailed health datasets should be treated as if they have name and address in the clear, both practically and in our governance, regulatory, and legislative frameworks\u0026rdquo;\nBetter research into privacy #   \u0026hellip;teams managing data access requests in organisations such as NHS Digital and elsewhere are commonly applying rules of thumb, or intuition, to inform their proportionate responses. It would be better for the whole community ‚Äì patients, researchers, and those serving their needs ‚Äì if this were addressed with a modest programme of applied health research from a national funder\n Recommendations #   There is no new emergency, but TREs should be used, and data dissemination should not expand UKRI/NIHR should resource applied methods research into privacy preservation Revise the definitions of ‚Äòanonymous‚Äô, ‚Äòidentifiable‚Äô and ‚Äòlinked‚Äô data; add a new category of ‚Äòpseudonymised but re-identifiable‚Äô  "},{"id":17,"href":"/notebook/docs/analyst-skills/skills/","title":"Skills","section":"Analyst skills","content":"Skills #  A full description of a typology of skills and an associated competency framework is out of the scope of this work, nonetheless a rough framework is presented here to give structure to the discussion about the types of training that might be useful. This can be added to in the future when more national work has been carried out with regards to competency frameworks and careers in health and care analytics. There are several dimensions of interest in this context:\n Health and care domain knowledge (MH, acute, maternity\u0026hellip;) Analytic knowledge (stats, ML, R\u0026hellip;) Analytic task (demand and capacity, population health management)  Health and care domain knowledge #  It may not be useful to list these exhaustively since there are so many possibilities and levels of hierarchy that could be used to describe this, but it might be worth describing the domains broadly.\nAnalytic knowledge #  This is the level that this work most obviously concerns and warrants a reasonably comprehensive description (although not at the level forthcoming from national work on competency frameworks)\nAnalytic task #  As well as dataset type (acute, mental healthcare, etc.) there are domains of analytic task. The SAIU has been restructured to include the following interest groups:\n Demand and Capacity Population Health Management Health Inequalities Transformation and Efficiency Place based partnerships  This is thought to provide a useful structure to the current piece of work.\n"},{"id":18,"href":"/notebook/docs/data_saves_lives/","title":"Data Saves Lives","section":"Docs","content":"Data saves lives: reshaping health and social care with data #  This report will be quoted and remixed according to the terms of the Open Government Licence- as always I would like to express my gratitude that this is being used because reading, summarising, quoting, and citing stuff that doesn\u0026rsquo;t have an open licence can be a real pain in the neck.\nIntroduction #   The future of the NHS depends on improving how we use data for 4 related purposes:\n For the direct care of individuals. To improve population health through the proactive targeting of services. For the planning and improvement of services. For the research and innovation that will power new medical treatments.   (Dr Tim Ferris, National Director of Transformation, NHS England)\nThis report consists of seven chapters:\n Improving trust in the health and care system‚Äôs use of data Giving health and care professionals the information they need to provide the best possible care Improving data for adult social care Supporting local and national decision-makers with data Empowering researchers with the data they need to develop life-changing treatments, diagnostics, models of care and insights Working with partners to develop innovations that improve health and care Developing the right technical infrastructure  "},{"id":19,"href":"/notebook/docs/analyst-skills/training/","title":"Training","section":"Analyst skills","content":"Training #  Within this piece of work the following training needs have been identified:\n TODO  A summary of available training in these areas follows.\nArea one #   TODO  Appendix A: Training providers #   AphA Health Education England Government Statistics Service NHS R Community Midlands Decision Support Network FutureNHS Healthcare Evaluation Data Kurtosis Skills Development Network Operational Research Society QA EDX WiseOwl FutureLearn Laria Udemy Jumping Rivers Population Health Exchange NIHR CLAHRC North Thames Faculty of Public Health Public Health England East Midlands AHSN Strategy Unit Software carpentry PenARC Mango  "},{"id":20,"href":"/notebook/docs/datascience/","title":"Data science in the NHS","section":"Docs","content":"Data science in the NHS #  The GOV.UK service standard is widely regarded as representing good practice in delivering digital services in the public sector. It is mandatory for certain digital services within government but the latest guidance includes standards which can be adopted across the public sector regardless of whether they would require mandatory assessment. It includes fourteen standards.\nThis document examines these standards from the point of view of data science teams, and particularly data science teams within the NHS. The NHS service standard reproduces the 14 standards, and provides extra information to teams in the NHS in recognition of some of the ways that the NHS can differ from other areas of the public sector. The NHS service standard also includes 3 extra service standards, again recognising the unique environment of the NHS.\nExamples of the difference between working within the NHS and the rest of the public sector include:\n multi-disciplinary teams are not common in the NHS - by \u0026ldquo;multi-disciplinary\u0026rdquo; we mean teams made up of product and delivery managers, designers, developers, user researchers and content designers NHS delivery teams are less likely to be practising user-centred design and agile service delivery measuring outcomes is often more complex for health more products and services are commissioned locally, for example in hospital trusts, and they are more likely to rely on suppliers, long-term contracts and \u0026ldquo;off the shelf\u0026rdquo; solutions  These issues are clearly equally important in data science, particularly the last point, and I propose in this document that data science teams across the NHS begin to work more closely in line with the GOV.UK service standard, as expressed within the NHS digital service standard and including the extra three points on it. The fourteen GOV.UK points area listed following, followed by the three extra NHS points.\n Understand users and their needs in the context of health and care Work towards solving a whole problem for users Provide a joined up experience across all channels Make the service simple to use Make sure everyone can use the service Create a team that includes multidisciplinary skills and perspectives Use agile ways of working Iterate and improve frequently Respect and protect users' confidentiality and privacy Define what success looks like and be open about how your service is performing Choose the right tools and technology Make new source code open Use and contribute to open standards, common components and patterns Operate a reliable service Support a culture of care Make your service clinically safe Make your service interoperable  "},{"id":21,"href":"/notebook/docs/analyst-skills/resources/government_analysis_function/","title":"Government Analysis Function","section":"Resources","content":"Government Analysis Function (AF) Learning Curriculum #  Summary #  This document is designed for analysts across the civil service. It is available under the OGL3.\nIt is divided into five areas:\n Ability  Gain aptitude or potential to perform to the required standard ‚Äì ‚ÄúI need to learn how to do something now‚Äù   Technical  Demonstrate specific professional skills, knowledge or qualifications ‚Äì ‚ÄúI want to get better at doing something‚Äù   Behaviours  Actions and activities that people do which result in effective performance in a job ‚Äì ‚ÄúI want to change a behaviour‚Äù   Strengths  The things we do regularly, do well and that motivate us ‚Äì ‚ÄúI want to learn from the experiences and strengths of others‚Äù   Experience  The knowledge or mastery of an activity or subject gained through involvement in or exposure to it ‚Äì ‚ÄúI want to experience something that will help me grow‚Äù    70 : 20 : 10 learning model #  The learning opportunities on the Learning Curriculum are grouped according to the 70:20:10 Learning Model. This framework suggests that in the workplace:\n 70% of learning is experiential. It happens through daily tasks, challenges and practice 20% of learning is social. It happens with and through other people, like colleagues 10% of learning is formal. It happens through structured training courses and programs  This is a key insight in the current context and elsewhere and illustrates how important it is to:\n give analysts opportunities to practise their skills give analysts the opportunity to network and learn from each other  Formal training makes up a very small proportion of the value in learning.\nTechnical #  The technical part of the curriculum lists the following topics:\n Data analysis Data management Data science Data visualisation Economics Geography Modelling tools and techniques New systems and ways of working Operational research Quality assurance, validation and data linkage Software programming, tools and techniques Survey design  "},{"id":22,"href":"/notebook/docs/opensource/","title":"Open source","section":"Docs","content":"Open source #  I constantly talk about open source and regurgigate the same links to policy and best practice over and over again so it has finally dawned on me that I should just write it down once and then, you guessed it\u0026hellip; open source it. So here it is.\n(I may expand this into a whole section in time but let\u0026rsquo;s start with the basics)\nPolicy #   Guidance on sharing code from GDS: https://www.gov.uk/service-manual/technology/making-source-code-open-and-reusable The NHS version of the above: https://service-manual.nhs.uk/service-standard/12-make-new-source-code-open Sharing code being talked about in NHSX‚Äôs data saves lives strategy document: https://www.gov.uk/government/publications/data-saves-lives-reshaping-health-and-social-care-with-data-draft/data-saves-lives-reshaping-health-and-social-care-with-data-draft#helping-developers-and-innovators-to-improve-health-and-care  Guidance #   Security and open code Why code in the open When code should be open or closed Health Foundation on opening their code NCSC Secure development and deployment guidance This RAP repo contains loads of excellent material on git and open NHSX on open code  Awesome repos that show how it\u0026rsquo;s done #  (although note some of these are imperfect e.g. they lack licences)\n The superb, world leading coronavirus dashboard from PHE https://github.com/publichealthengland/coronavirus-dashboard An analysis of risk factors for COVID deaths across 17 million linked NHS records https://github.com/opensafely/risk-factors-research Open prescribing, an API/ dashboard for GP level prescription data https://github.com/ebmdatalab/openprescribing A tool to calculate growth centiles for children, developed by RCPCH https://github.com/rcpch/digital-growth-charts-server The NHSX/ NHSR funded R wrapper to our locally developed tool to classify text based patient experience data (this is from my team üòä) https://github.com/nhs-r-community/pxtextmineR An experimental bed allocation tool from NHSX https://github.com/nhsx/skunkworks-bed-allocation Open source SQL that produces the MHSDS from NHSE https://github.com/nhsengland/MHSDS  (This is really just a completely rushed random snapshot. If you know about something cool in open source health and social care, get in touch or make a PR ‚ò∫Ô∏è)\n"},{"id":23,"href":"/notebook/docs/goldacre_review/nottshc/","title":"Nottinghamshire Healthcare NHS Trust","section":"Goldacre Review","content":"What should Nottinghamshire Healthcare NHS Trust do #  Nottinghamshire Healthcare NHS Trust (NHC) is part of an analytically able ICS and employs three analytical teams with good working relationships: Applied Information, Performance, and CDU data science. The teams combine expertise in data curation and management, performance and quality surveillance, and data science methods and analytical pipelines.\nNHC could become one of the \u0026ldquo;data pioneer\u0026rdquo; trusts (or form part of a \u0026ldquo;data pioneer\u0026rdquo; ICS) that the report recommends in NHSA 7 by building on the work already happening at the trust.\nReproducible analytical pipelines #  TODO\nOutsourcing #  TODO\nOpen code #   Actively seek permission to open source code that refers to database schema from proprietary databases. OpenSAFELY publishes code from databases produced by TPP (with their permission), and NHC should do the same, as well as approaching the other main EHR database vendor the trust has engaged, Servelec (regarding the Rio database)  TODO\n"},{"id":24,"href":"/notebook/docs/analyst-skills/competency_frameworks/","title":"Competency Frameworks","section":"Analyst skills","content":"Competency frameworks #  Introduction #  APHA carried out a review of competency frameworks for health service analysts, published in 2021. There were 28 competency frameworks considered in the review, of which 10 were found to contain significant detail about analytical job roles and career progression. Some of the material from this review and from the competency frameworks is presented below.\nThey assessed four frameworks as being the highest ranked in the scoring that they designed:\n Government Analysis Career Framework NHS KSF AphA Professional Registration Criteria Civil Service Framework  They note that despite the high score the skills within the KSF relating to analytics are out of date and that framework will not be discussed any further here.\nOther notable frameworks they reviewed include:\n The Clinical Informatics Framework The digital, data, and technology profession capability framework EDISON Data science framework  The digital, data, and technology profession capability framework #  This framework is reviewed elsewhere in this notebook from the perspective of the synergy of the roles within it with the data science family.\n"},{"id":25,"href":"/notebook/docs/goldacre_review/nhs-r/","title":"What has NHS-R done and what else should it do?","section":"Goldacre Review","content":"Where does NHS-R fit? #  Brilliantly, NHS-R is already leading on many of the things from this review that are important to me. I\u0026rsquo;m going to highlight the stuff we already started doing, with nobody asking us to, and a paltry amount of funding, and then I\u0026rsquo;m going to talk about the stuff that we can make a really solid contribution to tomorrow if we\u0026rsquo;re brought in to the conversation (which, according to the review, we should be).\n[if you\u0026rsquo;re reading this and you\u0026rsquo;re not me, these are just notes for now. I will make it less scrappy]\nWhat has NHS-R already done? #  Modernising NHS service analytics #   NHSA 4. Support an NHS analyst community  Slack Twitter Conference GitHub   NHSA 5. Develop an annual data conference for NHS service analysts NHSA 18. Ensure all training is open by default  All NHS R training is open (and open licensed) because of course it is  Intro Interactive intro Shiny Git [bare repo at time of writing but in active development ready for conference November 2022] RMarkdown     NHSA 19. Create and maintain a curated national open library of NHS analyst code  A wonderful open repo full of all kinds of useful stuff in the R language    Open working #   Open 3. Make open code a boilerplate feature of all public contracts  NHS-R already did this for NHS-R solutions, as a consequence there are tens of properly licensed repos on the NHS-R GitHub   Open 14. Write an ‚Äòopen analytics policy for the NHS‚Äô  We already started doing this and would be happy to contribute to work in DHSC and NHST   Open 18. Create a ‚ÄòCode For Health‚Äô training programme for NHS service analysts and academic researchers  NHS-R is mentioned by name, we have already done a huge amount of work training analysts in R and related disciplines (such as git)   Open 31. Provide guidance and training on RAP and code sharing  Another recommendation addressed by the statements on tools repo Git training is forthcoming (expected November 2022)    What can NHS-R do in the future? #  Modernising NHS service analytics #   NHSA 4. Support an NHS analyst community  Coffee and coding   NHSA 11. Devise the content of a national training programme for NHS analysts: initial and CPD  NHS-R wouldn\u0026rsquo;t be expected to contribute across the board, but NHS-R includes many experts in:  RAP dashboard programming programming for statistics and machine learning Git, GitHub, and collaboration Training in data science methods     NHSA 12. Oversee funding and delivery of training, both open online and one-to-one  Open working #   Open 18. Create a ‚ÄòCode For Health‚Äô training programme for NHS service analysts and academic researchers  NHS-R is mentioned in this recommendation- NHS-R should continue to provide readily accessible help to those using open source data science methods in their work (experiential learning) as well as contribute to formal training programmes inside and outside of NHS-R NHS-R could consider more \u0026ldquo;MOOC\u0026rdquo; like approaches to training (\u0026ldquo;In-person supervision is necessary for a subset of attenders, especially those seeking certification, to supervise practical work, and for marking work to evaluate competences\u0026rdquo;) and should be funded and supported to do so    Summary #  Broadly, three main areas of activity are identified above:\n Community building  Two weekly drop in sessions Regular updates on GitHub issues and work triage More involvement of and communication with fellows Regular, structured communication programme featuring tweets, blogs, and podcasts   Experiential learning  Mentoring scheme NHS data science acceletator launched Run and co-run NHS-R training NHS-R solutions can include a component where training/ supervising/ assisting the team delivering them is included   Writing code, software, and training  Offer an \u0026ldquo;out of the box\u0026rdquo; NHS-R solution where software is provided to organisations that request it   Contributing to policy  "},{"id":26,"href":"/notebook/docs/analyst-skills/data_collection/","title":"Data Collection","section":"Analyst skills","content":"Data collection #  Original PHE data collection #  Public Health England carried out a Team Level Population Health Intelligence Skills Mapping within Nottinghamshire in 2018. The tool mainly comprised data collection which described the number of full time staff within each organisation with skills in the following areas:\n Database design, development and operations Data sharing and information governance Data linkage Data sources for population health intelligence Database analysis Routine monitoring Analytics Predictive analytics Benchmarking and measuring variation Statistical analysis Research and evaluation skills Data science skills Data visualisation Communication to different audiences Consultancy skills Options appraisal Population health approaches  Skills were defined at four levels:\n 0: No experience of this skill group or do not need this skill set within their role (this can be all of the team) 1: Has an understanding of this group of skills 2: Using the skills outlined in this competency regularly as a core component to the work that they do 3: High level of expertise in this work area and are supervising or training/ teaching other staff within this area  Repeating this analysis was not felt to be productive on its own, however a more detailed set of data was collected which contained the original data spec- with the repeated part of that data collection shared back with PHE.\nNew data collection #  Domains were added to the data collection instrument based on the five areas of interest identified within the ICS:\n Demand and Capacity Population Health Management Health Inequalities Transformation and Efficiency Place based partnerships  One other domain was added to the data collection instrument based on discussions within the project group:\n Knowledge of healthcare domains, datasets, and clinical coding systems  Population health management was not added because it already existed within the original tool. Economic analysis was also identified as an area of interest but is covered in the original tool under Options appraisal. Arguably it would be better to disaggregate options appraisal to leave health economic analysis as a separate domain, but keeping the tool broadly in line with the previous PHE led exercise was felt to be valuable and so it was left in its original form.\nAdding time spent using skills #  As well as the extra other domains, an extra measure was added to each domain, which would assess the amount of time actually spent on the different domain areas. Estimating the actual time spent on 20+ domains was not felt to be a reasonable request and so four levels of use were defined:\n 0: Never 1: Once or twice a year 2: Monthly 3: Weekly  "},{"id":27,"href":"/notebook/docs/deidentification/","title":"Deidentification","section":"Docs","content":"Deidentification of data #  The team and I do a lot of stuff with patient experience data  and it can be tricky to reuse it without thoroughly redacting it because people put personal information in there- sometimes asking for a response, by email or telephone. It\u0026rsquo;s a bit of an unusual case because the data should be anonymous- there\u0026rsquo;s no need to identify patients in it at all, I just want the text of their experience, but in practice it isn\u0026rsquo;t.\nHaving some way of automatically redacting patient experience data to a high enough standard to reuse the data would be very useful, and to that end I made a pull request on the NHSX internship site to see if they could look at it with a PhD student.\nJonny Pearson very helpfully gave me some links to some resources to improve it and I need to digest the contents of a bit, may as well do it here in case it helps anyone else (or future me, for that matter üòÑ). We have:\n Introduction to anonymisation Anonymisation: managing data protection risk code of practice A Review of Anonymization for Healthcare Data  You may just want the links and ignore my witterings below of course üòâ.\nIntroduction to anonymisation #  At a simple level, organisations who are trying to anonymise data will mask direct identifiers. Some data, like diagnosis or medication is not identifiable. Some data, like name or phone number is identifiable, and these variables are known as direct identifiers. Masking the direct identifiers should in theory make it impossible to identify individuals, but Sweeney showed that combining quasi identifiers could lead individuals to be identified. Quasi identifiers are values that are not directly identifying on their own, but may be identifying in combination. Sweeney suggested that this kind of linkage attack can be prevented by generalising some of the data. For example, using year and month of birth rather than exact birth date.\nSweeney codified these ideas within a concept called k-identity. The k variable gives the minimum number of people who are indistinguishable from each other in the dataset. A k-anonymous dataset means that each row is indistinguishable from k-1 other rows. For example, when k is 5 each individual is indistinguishable from 5 other individuals.\nThe modern big data landscape #  In more recent times the data available about each individual has proliferated and additional controls are often required in order to thoroughly anonymise data. Controlling the environment can help with this, for example by having data accessed in a secure area where data access is monitored, or providing data in a secure way that doesn\u0026rsquo;t allow it to be recombined with other data, like a trusted research environment.\nEnvironmental controls do not generally affect the utility of data for analysis, but have an associated cost, control of use, etc.\nSummary #   Deidentification comes at a cost- either in terms of the usefulness of the data or the cost of managing environmental access to data Deidentification can be very difficult when there is high dimensional or high frequency data It is not always obvious what will and what won\u0026rsquo;t lead to reidentification, and there are many high profile failure of deidentified data (e.g. Netflix)  Aggregation #  "},{"id":28,"href":"/notebook/docs/new_power/","title":"New Power","section":"Docs","content":"New Power/ Old Power #  Heimans and Timms (2014) discussed the differences between what they called \u0026ldquo;Old power\u0026rdquo; and \u0026ldquo;New power\u0026rdquo;. All illustrations are sourced from Harvard Business Review and are reproduced under fair use.\n"},{"id":29,"href":"/notebook/docs/analyst-skills/conclusion/","title":"Conclusion","section":"Analyst skills","content":"Conclusion #  Scope #  In order to carry out this work within a reasonable timeframe it was scoped to exclude any detailed work on career and competency frameworks (which are in any case forthcoming nationally).\n(Are we going to add any more scope to this- see Appendix A?)\nOutline #  The current piece of work is designed to carry out the following functions as presented in the introduction:\n Consider the types of analyst teams within the ICS and their diverse functions Identify a generic set of skills for healthcare analysts in the region Identify extra skills necessary dependent on role and organisation Identify individuals within the ICS who can provide training on these skillsets Identify gaps where the system is not self sufficient for training and procure training (preferring free training) Look at the range of external training available to the system Identify areas of training need that cannot be met internally or externally  These areas will each now be considered in turn.\nConsider the types of analyst teams within the ICS and their diverse functions #  The type and function of analytic teams across the ICS is clearly broad. They might be though of as falling roughly into one of four types:\n Performance Data analysis/ reporting Data science Public health/ population health  In turn those broad kinds of teams might work on different sorts of problems and with different approaches, the teams are therefore considerably more diverse in terms of skillset and function than might be concluded from looking at the four types. This is discussed further under skills.\nIdentify a generic set of skills for healthcare analysts in the region #  Due to the wide range of types of team and task in the region, generic skills are relatively few. It may be worthwhile, however, to define them very broadly and then to consider in more detail within particular contexts.\n Data visualisation  Whether or not the visualisations are presented to the enduser, all individuals who process data should have basic data viz skills, to allow them to rapidly check a dataset for outliers, conduct exploratory analyses, etc.   Software skills  In its broadest possible sense, data analysts need to have software skills, whether they be those relating to Excel, R, Python, PowerBI, and developing those skills can make analysts more efficient and more effective   Communication  Analysts need to communicate effectively, whether that be through the written and spoken presentations of data or in conversations with other staff relating to problem definition, hypothesis generation, resource allocation, or other matters   Hypothesis testing  In its broadest possible sense, analysts need to test hypotheses. This does not have to be done formally or even with a technique such as SPC but analysts should consider the truth values of statements and attempt to derive them from data    Identify extra skills necessary dependent on role and organisation #  The skills that depend on role and organisation are by their nature quite diverse. We can usefully think about analytic work in terms of five types:\n Demand and Capacity Population Health Management Health Inequalities Transformation and Efficiency Place based partnerships  These analytic tasks will be carried out within different domains:\n Social care (subdivisions?) Acute Mental health Community physical health care Primary care  (see also Appendix A for another analytic taxonomy that might merit inclusion)\nIdentify individuals within the ICS who can provide training on these skillsets #  TODO\nIdentify gaps where the system is not self sufficient for training and procure training (preferring free training) #  TODO\nLook at the range of external training available to the system #  TODO but see this summary of external training providers\nIdentify areas of training need that cannot be met internally or externally #  Appendix A: Decisions to be made #  Where is the focus?\n ICS Strategic Analytics team The strategic analytical needs of the ICS to be met by the above and contributions from partner organisation teams All analysts in an ICS meeting ICS and organisational needs  [I think largely 2 + 3, but for discussion]\nWhat types of analysis are we considering?\n clinical decision-making to help busy clinicians diagnose and manage disease innovation and change in the NHS, and to evaluate the success of new models of care and whether changes deliver the expected benefits effective board-level oversight of complex organisations and care systems better everyday management of the monitoring and improvement of the quality and efficiency of care senior decision-makers to respond better to national incentives and regulation the allocation of finite resources better understanding of how patients flow through the system new data and digital tools patients and the public in using information  Method of data collection for analyst skills. I would propose that we carry out some sort of review of individuals (could be completed by line managers) which would summarise:\n Skillset Activities (70% of the 70 : 20 : 10 rule) Areas where the analyst might form part of a community of practice (20% of the 70 : 20 : 10 rule) Areas where the analyst can share knowledge with the ICS (10% of the 70 : 20 : 10 rule)  "},{"id":30,"href":"/notebook/docs/goldacre_review/further-work/","title":"Further Work","section":"Goldacre Review","content":"Further work #  I love this review and it\u0026rsquo;s probably the best blueprint for real change that I\u0026rsquo;ve seen, although it\u0026rsquo;s worth mentioning that I\u0026rsquo;ve seen 10+ such documents that have not led to real change, not because of deficiencies on their part but because of ignorance, apathy, and unchecked commercial interests that have pushed methodologically inferior and expensive tools and methods to senior managers who don\u0026rsquo;t know what they\u0026rsquo;re buying.\nClearly in a document of this size every detail will not be covered, so here is a non exhaustive list of things that I would like to see further work on now the report has been published.\nPublishing code derived from databases #  I have an open issue about this on the NHSX open code policy repo. My work involves using data that comes from off the shelf databases, and the code that we write quite naturally includes database schemata and variable names. The guidance from gov.uk is very clear:\n You should keep some data and code closed, including:\n keys and credentials algorithms used to detect fraud unreleased policy   [\u0026hellip;]\n You should open all other code. This includes:\n configuration code database schema security-enforcing code   Nonetheless, I do not have permission to publish anything that implies the structure or even the variable names of any of the databases that we work with. This makes publishing some of our best code difficult or impossible. There are two objections given:\n Publishing table names is a security risk The database vendors own the \u0026ldquo;IP\u0026rdquo; of the schema and even the variable names and we are therefore not permitted to publish this information.  I have asked a vendor about this second issue and they refuse to talk to me. They will only talk to my Trust\u0026rsquo;s representative; my Trust refuses to ask them for this information.\nI desperately need crystal clear guidance on the legal and technical aspects of this issue from inside the NHS so I can win this argument once and for all. NHSX\u0026rsquo;s open code guidance is great but as the linked issue makes clear it does not currently have anything to say on this issue. Hopefully it will in the future.\nSoftware licensing #  There is a section addressing \u0026ldquo;myths\u0026rdquo; about open working in the section on open working and it is very welcome and sensible. There is quite a large issue lurking beneath the surface which I would like to highlight. The report reads:\n Adopting open working practices does not mean other countries or industry can exploit intellectual property created with state funds: there should be a robust and thoughtful exceptions framework to impose commercial licenses or restrictions on review and (separately) re-use of publicly funded code, where this is actively helpful; but this closed approach should be used in a planned and deliberate fashion, where it meets national strategic objectives, not as the unplanned default approach\n I have heard this objection to open code many times, and have tried to address it where I can. On the face of it it seems very sensible: share the code with, say the NHS, and exploit the code commercially where it can be exploited commercially. The original text is not entirely clear but I am assuming that they are referring to dual licensing of code. I have filed an issue on NHSX\u0026rsquo;s open code guidance on this subject. I am no expert but I am dubious as to whether the benefits of dual licensing code can be realised in the NHS. It\u0026rsquo;s a notoriously tricky area and my fear is that dual licensing will\na) scare off people who would otherwise have contributed, because they don\u0026rsquo;t work in an organisation which can use a free licence and so would have to pay to use a project they themselves contributed to\nb) will not produce anywhere near the revenue to make all the administration worthwhile- companies which can\u0026rsquo;t use a free licence will either just reimplement the code themselves or just use a truly free alternative.\nSecurity and information governance #  The \u0026ldquo;platforms\u0026rdquo; section of Barriers to RAP in Open working rather states \u0026ldquo;It is reasonable for local IT teams to be cautious, especially for more versatile tools such as Python, and especially when staff are working with more sensitive data\u0026rdquo;, but gives no further details.\nIt is unclear what the intention of this statement is. What is caution in this case? Not allowing installation at all? And what is covered other than Python? R? Julia? JavaScript? What qualifies as \u0026ldquo;sensitive data\u0026rdquo;? Any row level data at all?\nInterpreted literally this statement appears to mean \u0026ldquo;Python should never be used to interact with row level patient data on a computer issued by a typical NHS organisation [such as a provider trust]\u0026rdquo;. I don\u0026rsquo;t think that can possibly be the intention (?) but I\u0026rsquo;m not clear.\nRegardless, there is a clear need for crystal clear guidance on security and information governance, which will cover things like:\n Language (R, Python, Julia, etc.) Libraries (E.g. The R-based CRAN store of packages is scrutinised by a volunteer team, by contrast Python\u0026rsquo;s PyPI is not and you can deploy anything you like there) Docker, Linux, hosting of analytic products (e.g. using Azure or RStudio) in the cloud/ behind the firewall Should some users be allowed access to software (like Python) that others are not? Should some users have (some) admin privileges on their machine where other users are not given the same privileges? Is free software intrinsically less safe or trustworthy than paid software? Clearly completely unknown software from an unknown individual or organisation is less trustworthy than R, but how do IT teams learn to make judgements like this when they are not familiar with the software from their work?  "},{"id":31,"href":"/notebook/docs/analyst-skills/resources/","title":"Resources","section":"Analyst skills","content":"Resources #  Please note the resources are reproduced in the repo with permission and no permission to reproduce elsewhere is implied (although asking nicely usually works, in my experience üòâ).\nSources include:\n Advancing analytics in the NHS Recommendations for advancing the analytical capability of the NHS and its ICS partners Government analysis function learning curriculum  Appendix- other resources #  This appendix is to record other resources which were not examined in detail due to lack of time.\n Organizing for Success Part III: How to Organize and Staff Data Analytics Teams  "},{"id":32,"href":"/notebook/docs/hugo/","title":"Hugo","section":"Docs","content":"While I remember, here is a very quick list of things to do when you\u0026rsquo;re using Hugo to make websites so they work properly.\n Configure config.toml so that the baseURL points to the actual location- either online or in your file system  This doesn\u0026rsquo;t sound like much but it won\u0026rsquo;t render properly unless you do this- even if using hugo server does render nicely   Install dependencies if necessary with npm install  I\u0026rsquo;m using three different themes, I\u0026rsquo;ll list them all and talk about how to set them up another time\n Terminal Geek docs Notebook (this one)  "},{"id":33,"href":"/notebook/docs/github/","title":"GitHub","section":"Docs","content":"GitHub #  View the source and make a pull request.\n"}]